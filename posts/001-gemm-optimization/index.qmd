---
title: "Note 001: GEMM Optimization"
description: "Iterating on General Matrix Multiplication in CUDA for optimal performance on NVIDIA GPUs"
date: "2026-02-02"
categories: [CUDA, GEMM, Linear Algebra]
image: image.png # You can add a preview image later
draft: false 
format:
  html:
    code-annotations: hover
---

## Introduction

General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs. 

### Mathematical definition

Formally, GEMM is defined as an operation on two input matrices $A$ and $B$, and an accumulation matrix $C$, scaled by scalars $\alpha$ and $\beta$:

$$
C = \alpha \cdot (A \times B) + \beta \cdot C
$$

Where:

* $A$ is an $M \times K$ matrix.
* $B$ is a $K \times N$ matrix.
* $C$ is an $M \times N$ matrix.

![](images/01_matrix_dims.svg){width=80%}

In deep learning contexts, $\beta$ is often 0 (overwriting the output) or 1 (accumulating gradients), and $\alpha$ is typically 1.

### Why GEMM?

In modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: $\text{softmax}(\frac{Q \times K^T}{\sqrt{d}}) \times V$. Aside from the softmax operation, everything else can be represented as GEMM:

1.  Calculating the scaled attention scores ($\frac{Q \times K^T}{\sqrt{d}}$).
2.  Calculating the weighted sum of values ($\text{scores} \times V$).

Since GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale.

### Problem setup
As I iterate on GEMM kernels, I will test them on the General Matrix Multiplication test suite and infrastructure on LeetGPU.com. As per the problem setup there, I will only be using native capabilities of the GPUs, so no libraries like CuTe or cuBLAS. The test suite is hidden, but the known constraints are that each of the matrix dimensions $M$, $N$, and $K$ are between 16 and 4096. So the input matrices range from very small (a few hundred elements) to fairly large (16 million elements). The input matrices A and B are given as type half (half-precision floating point number). Lower than usual precision floats are common in AI workloads as they take up less space and allow for higher throughput. For improved accuracy, the computation of the GEMM output will be done using full-precision floats, but the final storage will also be as a half-precision float.

For each kernel, I will explain the algorithm, how it interacts with the GPU architecture and memory hierarchy, and show the full code in CUDA C++. Finally, I will discuss the computational intensity of the kernel and benchmark its performance on the following NVIDIA GPUs: Tesla T4 (2017), Ampere A100-80GB (2020), Hopper H100 (2022), Hopper H200 (2023), and Blackwell B200 (2024). 

### Assumed background
I will assume the reader has at least the following understanding of the CUDA programming model: as a programmer, when we ask the GPU to execute a CUDA kernel, we first decide on the dimensions of a grid of threads that will be launched. Threads are grouped into blocks, and multiple blocks are assigned to each Streaming Multiprocessor (SMs). A block is resident on an SM until all of its threads have completed, at which point it is evicted. Multiple blocks can be resident at a time on an SM, and the SM will execute whichever threads are ready for compute and not waiting for memory, in order to hide memory latency. GPUs have several types of memory: registers, shared memory, L1 and L2 caches, constant memory, and global memory. Using these types of memory effectively is crucial for maximizing performance. Performance can vary widely based on the specific architecture of the GPU, and we can understand how to improve a kernel by calculating its arithmetic intensity and checking whether the kernel is compute or memory bound. 

If the reader is not familiar with these concepts, I refer them to Programming Massively Parallel Processors (Kirk & Hwu), an excellent resource that explains these concepts much better than I could in this post. Chapters 1 through 6 cover everything listed above.



## 1. Naive Matrix Multiplication
In a naive parallel computing model, we can have every thread be solely responsible for computing exactly one output element in the final matrix. Each thread would load the row from A and column from B that it needs for the dot product for that output element. 

Hover over the numbered annotations for explanations of key parts.

### Annotated Code

```cpp
#include <cuda_fp16.h>
#include <cuda_runtime.h>

__global__ void gemm_naive_kernel(const half* A, const half* B, half* C, 
                                  int M, int N, int K, 
                                  float alpha, float beta) { // <1>
    
    // Calculate global row and column indices for this thread
    int col = blockIdx.x * blockDim.x + threadIdx.x; // <2>
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    
    // Boundary check: ensure we don't access memory outside the matrix
    if (row < M && col < N) {
        float val = 0;
        
        // The K-loop: Perform the dot product
        for (int i = 0; i < K; i++) {
            val += __half2float(A[row * K + i]) * __half2float(B[i * N + col]); // <3>
        }
        
        // Write result back to C
        val = alpha * val + beta * __half2float(C[row * N + col]);
        C[row * N + col] = __float2half(val);
    }
}

// Wrapper function to be called from Host
extern "C" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) { // <4>

    dim3 block(16, 16);
    // Grid calculation: ensures we cover the entire matrix (ceiling division)
    dim3 grid(
        (N + 15) / 16,
        (M + 15) / 16
    );

    gemm_naive_kernel<<<grid, block>>>(A, B, C, M, N, K, alpha, beta);
    cudaDeviceSynchronize();
}
```
1. **half vs. float**: We use `half` precision (FP16) for storage but perform accumulation in `float` (FP32). This is so that we can move data faster from global memory (only 2 bytes per element rather than 4), but during the accumulation computation, we don't lose small updates due to the smaller mantissa in FP16. (For example, imagine adding 0.01 to a running sum of 1000: if our mantissa is small enough, we may significantly alter or even omit some updates.)
2. **2D Indexing**: Standard mapping of a 2D thread block to matrix coordinates. `blockIdx` tells us which tile we are in; `threadIdx` tells us the pixel within that tile.
3. **The Bottleneck**: This line is the performance killer. For every single pixel in C, we are fetching the entire row of A and column of B from Global Memory (DRAM). 
4. **Host Wrapper**: `extern "C"` prevents C++ name mangling, allowing us to easily call this function from ctypes (Python) or PyTorch extensions later in the project.

### Roofline Model
For each output element of C, we load K elements of A and K elements of B in order to compute a dot product. For each pair of elements in the dot product, we multiply them together and then add the result to the running sum. Therefore, for every 2 halves we load from global memory (a total of 4 bytes), we perform 2 floating point operations. So our computational intensity is 2 FLOPs divided by 4 bytes, or 0.5 FLOP/B.

Below, we can see the roofline model for our test suite of GPUs, compared to the computational intensity of our first kernel. This kernel will be memory-bound on every GPU. Our first course of action to improve the performance of our kernel should be to rethink our memory access pattern.

Due to the hidden test suite, we technically don't know the FLOPS throughput of our kernel on each GPU. However we have the runtime, so we can look at that as a proxy, and compare the computational intensity of the kernel to the ridge point of each GPU to see how close we are to saturating the memory bandwidth or not. This should be accurate for determining if we are compute or memory bound.

| GPU Model | Memory Bandwidth | Peak FP16 Compute | Ridge Point (FLOP/Byte) | Runtime (ms) |
| :--- | :--- | :--- | :--- | :--- |
| **NVIDIA T4** | 320 GB/s | 65 TFLOPS | **203** | 8.49 |
| **NVIDIA A100 (80GB)** | 2,039 GB/s | 312 TFLOPS | **153** | 1.03 |
| **NVIDIA H100 (SXM)** | 3,350 GB/s | 989 TFLOPS | **295** | 0.54 |
| **NVIDIA H200 (SXM)** | 4,800 GB/s | 989 TFLOPS | **206** | 0.53|
| **NVIDIA B200** | 8,000 GB/s | 2,500 TFLOPS | **312** | 0.50 |

: Benchmark Targets: The Ridge Point represents the arithmetic intensity required to max out the GPU's compute. Below this number, the kernel is memory bound.


## 2. Tiled Matrix Multiplication


