{
  "hash": "a141fd20f99edeac9fd9e1514a94b6d7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Note 001: GEMM Optimization\"\ndescription: \"Iterating on General Matrix Multiplication in CUDA for optimal performance on NVIDIA GPUs\"\ndate: \"2026-02-02\"\ncategories: [CUDA, GEMM, Linear Algebra]\nimage: image.png # You can add a preview image later\ndraft: false \nformat:\n  html:\n    code-annotations: hover\n---\n\n## Introduction\n\nGeneral Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs. \n\n### Mathematical definition\n\nFormally, GEMM is defined as an operation on two input matrices $A$ and $B$, and an accumulation matrix $C$, scaled by scalars $\\alpha$ and $\\beta$:\n\n$$\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n$$\n\nWhere:\n\n* $A$ is an $M \\times K$ matrix.\n* $B$ is a $K \\times N$ matrix.\n* $C$ is an $M \\times N$ matrix.\n\n![](images/01_matrix_dims.svg){width=80%}\n\nIn deep learning contexts, $\\beta$ is often 0 (overwriting the output) or 1 (accumulating gradients), and $\\alpha$ is typically 1.\n\n### Why GEMM?\n\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: $\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V$. Aside from the softmax operation, everything else can be represented as GEMM:\n\n1.  Calculating the scaled attention scores ($\\frac{Q \\times K^T}{\\sqrt{d}}$).\n2.  Calculating the weighted sum of values ($\\text{scores} \\times V$).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale.\n\n### Problem setup\nAs I iterate on GEMM kernels, I will test them on the General Matrix Multiplication test suite and infrastructure on LeetGPU.com. As per the problem setup there, I will only be using native capabilities of the GPUs, so no libraries like CuTe or cuBLAS. The test suite is hidden, but the known constraints are that each of the matrix dimensions $M$, $N$, and $K$ are between 16 and 4096. So the input matrices range from very small (a few hundred elements) to fairly large (16 million elements). The input matrices A and B are given as type half (half-precision floating point number). Lower than usual precision floats are common in AI workloads as they take up less space and allow for higher throughput. For improved accuracy, the computation of the GEMM output will be done using full-precision floats, but the final storage will also be as a half-precision float.\n\nFor each kernel, I will explain the algorithm, how it interacts with the GPU architecture and memory hierarchy, and show the full code in CUDA C++. Finally, I will discuss the computational intensity of the kernel and benchmark its performance on the following NVIDIA GPUs: Tesla T4 (2017), Ampere A100-80GB (2020), Hopper H100 (2022), Hopper H200 (2023), and Blackwell B200 (2024). \n\n### Assumed background\nI will assume the reader has at least the following understanding of the CUDA programming model: as a programmer, when we ask the GPU to execute a CUDA kernel, we first decide on the dimensions of a grid of threads that will be launched. Threads are grouped into blocks, and multiple blocks are assigned to each Streaming Multiprocessor (SMs). A block is resident on an SM until all of its threads have completed, at which point it is evicted. Multiple blocks can be resident at a time on an SM, and the SM will execute whichever threads are ready for compute and not waiting for memory, in order to hide memory latency. GPUs have several types of memory: registers, shared memory, L1 and L2 caches, constant memory, and global memory. Using these types of memory effectively is crucial for maximizing performance. Performance can vary widely based on the specific architecture of the GPU, and we can understand how to improve a kernel by calculating its arithmetic intensity and checking whether the kernel is compute or memory bound. \n\nIf the reader is not familiar with these concepts, I refer them to Programming Massively Parallel Processors (Kirk & Hwu), an excellent resource that explains these concepts much better than I could in this post. Chapters 1 through 6 cover everything listed above.\n\n\n\n## 1. Naive Matrix Multiplication\nIn a naive parallel computing model, we can have every thread be solely responsible for computing exactly one output element in the final matrix. Each thread would load the row from A and column from B that it needs for the dot product for that output element. \n\nHover over the numbered annotations for explanations of key parts.\n\n### Annotated Code\n\n```cpp\n#include <cuda_fp16.h>\n#include <cuda_runtime.h>\n\n__global__ void gemm_naive_kernel(const half* A, const half* B, half* C, \n                                  int M, int N, int K, \n                                  float alpha, float beta) { // <1>\n    \n    // Calculate global row and column indices for this thread\n    int col = blockIdx.x * blockDim.x + threadIdx.x; // <2>\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    // Boundary check: ensure we don't access memory outside the matrix\n    if (row < M && col < N) {\n        float val = 0;\n        \n        // The K-loop: Perform the dot product\n        for (int i = 0; i < K; i++) {\n            val += __half2float(A[row * K + i]) * __half2float(B[i * N + col]); // <3>\n        }\n        \n        // Write result back to C\n        val = alpha * val + beta * __half2float(C[row * N + col]);\n        C[row * N + col] = __float2half(val);\n    }\n}\n\n// Wrapper function to be called from Host\nextern \"C\" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha,\n                      float beta) { // <4>\n\n    dim3 block(16, 16);\n    // Grid calculation: ensures we cover the entire matrix (ceiling division)\n    dim3 grid(\n        (N + 15) / 16,\n        (M + 15) / 16\n    );\n\n    gemm_naive_kernel<<<grid, block>>>(A, B, C, M, N, K, alpha, beta);\n    cudaDeviceSynchronize();\n}\n```\n1. **half vs. float**: We use `half` precision (FP16) for storage but perform accumulation in `float` (FP32). This is so that we can move data faster from global memory (only 2 bytes per element rather than 4), but during the accumulation computation, we don't lose small updates due to the smaller mantissa in FP16. (For example, imagine adding 0.01 to a running sum of 1000: if our mantissa is small enough, we may significantly alter or even omit some updates.)\n2. **2D Indexing**: Standard mapping of a 2D thread block to matrix coordinates. `blockIdx` tells us which tile we are in; `threadIdx` tells us the pixel within that tile.\n3. **The Bottleneck**: This line is the performance killer. For every single pixel in C, we are fetching the entire row of A and column of B from Global Memory (DRAM). \n4. **Host Wrapper**: `extern \"C\"` prevents C++ name mangling, allowing us to easily call this function from ctypes (Python) or PyTorch extensions later in the project.\n\n### Roofline Model\nFor each output element of C, we load K elements of A and K elements of B in order to compute a dot product. For each pair of elements in the dot product, we multiply them together and then add the result to the running sum. Therefore, for every 2 halves we load from global memory (a total of 4 bytes), we perform 2 floating point operations. So our computational intensity is 2 FLOPs divided by 4 bytes, or 0.5 FLOP/B.\n\nBelow, we can see the roofline model for our test suite of GPUs, compared to the computational intensity of our first kernel. This kernel will be memory-bound on every GPU. Our first course of action to improve the performance of our kernel should be to rethink our memory access pattern.\n\nDue to the hidden test suite, we technically don't know the FLOPS throughput of our kernel on each GPU. However we have the runtime, so we can look at that as a proxy, and compare the computational intensity of the kernel to the ridge point of each GPU to see how close we are to saturating the memory bandwidth or not. This should be accurate for determining if we are compute or memory bound.\n\n| GPU Model | Memory Bandwidth | Peak FP16 Compute | Ridge Point (FLOP/Byte) | Runtime (ms) |\n| :--- | :--- | :--- | :--- | :--- |\n| **NVIDIA T4** | 320 GB/s | 65 TFLOPS | **203** | TBD |\n| **NVIDIA A100 (80GB)** | 2,039 GB/s | 312 TFLOPS | **153** | TBD |\n| **NVIDIA H100 (SXM)** | 3,350 GB/s | 989 TFLOPS | **295** | TBD |\n| **NVIDIA H200 (SXM)** | 4,800 GB/s | 989 TFLOPS | **206** | TBD |\n| **NVIDIA B200** | 8,000 GB/s | 2,500 TFLOPS | **312** | TBD |\n\n: Benchmark Targets: The Ridge Point represents the arithmetic intensity required to max out the GPU's compute. Below this number, the kernel is memory bound.\n\n::: {#eea0760d .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.ticker as ticker\n\n# --- Configuration ---\n# GPU Specs: (Name, Bandwidth GB/s, Peak FP16 TFLOPS)\nGPUS = {\n    \"NVIDIA T4\": (320, 65),\n    \"A100 80GB\": (2039, 312),\n    \"H100 SXM\": (3350, 989),\n    \"B200\": (8000, 2500)\n}\n\n# Your Kernel Data (Update this as you optimize!)\n# Format: (Kernel Name, Arithmetic Intensity, Achieved TFLOPS)\nKERNELS = [\n    (\"Naive\", 0.25, 1.5),         # Example placeholder\n    (\"Tiled\", 50.0, 120.0),       # Example placeholder\n    (\"CuBLAS\", 153.0, 300.0)      # Example placeholder\n]\n\n# --- Plotting Logic ---\ndef plot_roofline():\n    fig, ax = plt.subplots(figsize=(10, 6))\n    \n    # X-Axis range (Arithmetic Intensity)\n    x = np.logspace(-1, 3, 100)  # From 0.1 to 1000 FLOP/B\n    \n    colors = ['#e0e0e0', '#bdbdbd', '#9e9e9e', '#757575'] # Grayscale for roofs\n    \n    # 1. Plot Rooflines\n    for i, (name, (bw, peak)) in enumerate(GPUS.items()):\n        # Memory Bound Line: y = Bandwidth * x\n        # (Convert BW to TFLOPS scale: GB/s * FLOP/B / 1e3 = TFLOPS)\n        y_mem = (bw * x) / 1000.0\n        \n        # Compute Bound Line: y = Peak\n        y_compute = np.full_like(x, peak)\n        \n        # The Roofline is the minimum of the two\n        y_roof = np.minimum(y_mem, y_compute)\n        \n        ax.plot(x, y_roof, label=f\"{name} Roof\", color=colors[i], linewidth=2, linestyle='--')\n        \n        # Annotate the \"Ridge\" point\n        ridge_x = (peak * 1000) / bw\n        if ridge_x < 1000:\n            ax.plot(ridge_x, peak, 'o', color=colors[i], markersize=4)\n\n    # 2. Plot Kernel Data Points\n    for name, intensity, tflops in KERNELS:\n        ax.plot(intensity, tflops, 'o', markersize=10, markeredgecolor='black', label=name)\n        ax.text(intensity * 1.1, tflops, name, fontsize=9, verticalalignment='bottom')\n\n    # Formatting\n    ax.set_xscale(\"log\")\n    ax.set_yscale(\"log\")\n    \n    ax.set_xlabel(\"Arithmetic Intensity (FLOPs / Byte)\", fontsize=11, fontweight='bold')\n    ax.set_ylabel(\"Performance (TFLOPS)\", fontsize=11, fontweight='bold')\n    ax.set_title(\"Roofline Model: FP16 Tensor Core Performance\", fontsize=14)\n    \n    ax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n    ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n    \n    # Format ticks to be readable\n    ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n    ax.yaxis.set_major_formatter(ticker.ScalarFormatter())\n    \n    plt.tight_layout()\n    plt.show()\n\nif __name__ == \"__main__\":\n    plot_roofline()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=949 height=565}\n:::\n:::\n\n\n## 2. Tiled Matrix Multiplication\n\nTo move beyond the naive implementation, we must address the primary bottleneck of GPU computing: **Memory Bandwidth**.\n\nIn a naive implementation, every thread loads its required row of $A$ and column of $B$ directly from global memory (HBM). This results in massive redundancy. For a tile of size $T \\times T$, the naive approach loads data $2T^2 \\times N$ times.\n\n**Tiling** exploits the data reuse inherent in matrix multiplication. By loading a small block of data into **Shared Memory** (a user-managed cache that is roughly 100x faster than HBM), threads in a block can repeatedly access the same data without hitting global memory.\n\n### Algorithm\n\nThe strategy is to divide the output matrix $C$ into tiles. A standard Thread Block (e.g., $16 \\times 16$) is responsible for computing one tile of $C$.\n\n1.  **Collaborative Load:** All threads in the block work together to load a sub-tile of $A$ and a sub-tile of $B$ from global memory into shared memory.\n2.  **Synchronization:** A barrier (`__syncthreads()`) ensures all data is loaded.\n3.  **Compute:** Threads calculate the partial dot product using the data in shared memory.\n4.  **Iterate:** The block moves to the next \"phase\" (the next chunk of the K-dimension) and repeats.\n\n## Visualizing the Data Flow\n\n### Memory Hierarchy\n\nBefore looking at the code, it is crucial to map the physical movement of data. We are explicitly moving data from the massive but slow High Bandwidth Memory (HBM) into the tiny but fast Shared Memory (SRAM).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}