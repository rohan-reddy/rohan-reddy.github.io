[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html",
    "href": "posts/001-gemm-optimization/index.html",
    "title": "Note 001: GEMM Optimization",
    "section": "",
    "text": "General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs.\n\n\nFormally, GEMM is defined as an operation on two input matrices \\(A\\) and \\(B\\), and an accumulation matrix \\(C\\), scaled by scalars \\(\\alpha\\) and \\(\\beta\\):\n\\[\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n\\]\nWhere:\n\n\\(A\\) is an \\(M \\times K\\) matrix.\n\\(B\\) is a \\(K \\times N\\) matrix.\n\\(C\\) is an \\(M \\times N\\) matrix.\n\n\nIn deep learning contexts, \\(\\beta\\) is often 0 (overwriting the output) or 1 (accumulating gradients), and \\(\\alpha\\) is typically 1.\n\n\n\n\n\nA Note on History The standardization of GEMM dates back to the BLAS (Basic Linear Algebra Subprograms) specification in the late 1970s and 80s.\nSpecifically, Level 3 BLAS (introduced in 1990) targeted matrix-matrix operations. This distinction was crucial: unlike vector-vector (Level 1) or matrix-vector (Level 2) operations, matrix-matrix operations have a higher ratio of arithmetic operations to memory accesses, allowing for much higher utilization of floating-point units.\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: \\(\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V\\). Aside from the softmax operation, everything else can be represented as GEMM:\n\nCalculating the scaled attention scores (\\(\\frac{Q \\times K^T}{\\sqrt{d}}\\)).\nCalculating the weighted sum of values (\\(\\text{scores} \\times V\\)).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale."
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#introduction",
    "href": "posts/001-gemm-optimization/index.html#introduction",
    "title": "Note 001: GEMM Optimization",
    "section": "",
    "text": "General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs.\n\n\nFormally, GEMM is defined as an operation on two input matrices \\(A\\) and \\(B\\), and an accumulation matrix \\(C\\), scaled by scalars \\(\\alpha\\) and \\(\\beta\\):\n\\[\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n\\]\nWhere:\n\n\\(A\\) is an \\(M \\times K\\) matrix.\n\\(B\\) is a \\(K \\times N\\) matrix.\n\\(C\\) is an \\(M \\times N\\) matrix.\n\n\nIn deep learning contexts, \\(\\beta\\) is often 0 (overwriting the output) or 1 (accumulating gradients), and \\(\\alpha\\) is typically 1.\n\n\n\n\n\nA Note on History The standardization of GEMM dates back to the BLAS (Basic Linear Algebra Subprograms) specification in the late 1970s and 80s.\nSpecifically, Level 3 BLAS (introduced in 1990) targeted matrix-matrix operations. This distinction was crucial: unlike vector-vector (Level 1) or matrix-vector (Level 2) operations, matrix-matrix operations have a higher ratio of arithmetic operations to memory accesses, allowing for much higher utilization of floating-point units.\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: \\(\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V\\). Aside from the softmax operation, everything else can be represented as GEMM:\n\nCalculating the scaled attention scores (\\(\\frac{Q \\times K^T}{\\sqrt{d}}\\)).\nCalculating the weighted sum of values (\\(\\text{scores} \\times V\\)).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rohan Reddy / Notes",
    "section": "",
    "text": "Rohan Reddy is a software engineer based in New York City.\nRead more about me â†’"
  },
  {
    "objectID": "index.html#recent-notes",
    "href": "index.html#recent-notes",
    "title": "Rohan Reddy / Notes",
    "section": "Recent Notes",
    "text": "Recent Notes"
  }
]