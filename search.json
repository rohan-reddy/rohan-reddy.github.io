[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html",
    "href": "posts/001-gemm-optimization/index.html",
    "title": "Note 001: GEMM Optimization",
    "section": "",
    "text": "General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs.\n\n\nFormally, GEMM is defined as an operation on two input matrices \\(A\\) and \\(B\\), and an accumulation matrix \\(C\\), scaled by scalars \\(\\alpha\\) and \\(\\beta\\):\n\\[\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n\\]\nWhere:\n\n\\(A\\) is an \\(M \\times K\\) matrix.\n\\(B\\) is a \\(K \\times N\\) matrix.\n\\(C\\) is an \\(M \\times N\\) matrix.\n\n\nIn deep learning contexts, \\(\\beta\\) is often 0 (overwriting the output) or 1 (accumulating gradients), and \\(\\alpha\\) is typically 1.\n\n\n\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: \\(\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V\\). Aside from the softmax operation, everything else can be represented as GEMM:\n\nCalculating the scaled attention scores (\\(\\frac{Q \\times K^T}{\\sqrt{d}}\\)).\nCalculating the weighted sum of values (\\(\\text{scores} \\times V\\)).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale.\n\n\n\nAs I iterate on GEMM kernels, I will test them on the General Matrix Multiplication test suite and infrastructure on LeetGPU.com. As per the problem setup there, I will only be using native capabilities of the GPUs, so no libraries like CuTe or cuBLAS. The test suite is hidden, but the known constraints are that each of the matrix dimensions \\(M\\), \\(N\\), and \\(K\\) are between 16 and 4096. So the input matrices range from very small (a few hundred elements) to fairly large (16 million elements). The input matrices A and B are given as type half (half-precision floating point number). Lower than usual precision floats are common in AI workloads as they take up less space and allow for higher throughput. For improved accuracy, the computation of the GEMM output will be done using full-precision floats, but the final storage will also be as a half-precision float.\nFor each kernel, I will explain the algorithm, how it interacts with the GPU architecture and memory hierarchy, and show the full code in CUDA C++. Finally, I will discuss the computational intensity of the kernel and benchmark its performance on the following NVIDIA GPUs: Tesla T4 (2017), Ampere A100-80GB (2020), Hopper H100 (2022), Hopper H200 (2023), and Blackwell B200 (2024).\n\n\n\nI will assume the reader has at least the following understanding of the CUDA programming model: as a programmer, when we ask the GPU to execute a CUDA kernel, we first decide on the dimensions of a grid of threads that will be launched. Threads are grouped into blocks, and multiple blocks are assigned to each Streaming Multiprocessor (SMs). A block is resident on an SM until all of its threads have completed, at which point it is evicted. Multiple blocks can be resident at a time on an SM, and the SM will execute whichever threads are ready for compute and not waiting for memory, in order to hide memory latency. GPUs have several types of memory: registers, shared memory, L1 and L2 caches, constant memory, and global memory. Using these types of memory effectively is crucial for maximizing performance. Performance can vary widely based on the specific architecture of the GPU, and we can understand how to improve a kernel by calculating its arithmetic intensity and checking whether the kernel is compute or memory bound.\nIf the reader is not familiar with these concepts, I refer them to Programming Massively Parallel Processors (Kirk & Hwu), an excellent resource that explains these concepts much better than I could in this post. Chapters 1 through 6 cover everything listed above."
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#introduction",
    "href": "posts/001-gemm-optimization/index.html#introduction",
    "title": "Note 001: GEMM Optimization",
    "section": "",
    "text": "General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs.\n\n\nFormally, GEMM is defined as an operation on two input matrices \\(A\\) and \\(B\\), and an accumulation matrix \\(C\\), scaled by scalars \\(\\alpha\\) and \\(\\beta\\):\n\\[\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n\\]\nWhere:\n\n\\(A\\) is an \\(M \\times K\\) matrix.\n\\(B\\) is a \\(K \\times N\\) matrix.\n\\(C\\) is an \\(M \\times N\\) matrix.\n\n\nIn deep learning contexts, \\(\\beta\\) is often 0 (overwriting the output) or 1 (accumulating gradients), and \\(\\alpha\\) is typically 1.\n\n\n\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: \\(\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V\\). Aside from the softmax operation, everything else can be represented as GEMM:\n\nCalculating the scaled attention scores (\\(\\frac{Q \\times K^T}{\\sqrt{d}}\\)).\nCalculating the weighted sum of values (\\(\\text{scores} \\times V\\)).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale.\n\n\n\nAs I iterate on GEMM kernels, I will test them on the General Matrix Multiplication test suite and infrastructure on LeetGPU.com. As per the problem setup there, I will only be using native capabilities of the GPUs, so no libraries like CuTe or cuBLAS. The test suite is hidden, but the known constraints are that each of the matrix dimensions \\(M\\), \\(N\\), and \\(K\\) are between 16 and 4096. So the input matrices range from very small (a few hundred elements) to fairly large (16 million elements). The input matrices A and B are given as type half (half-precision floating point number). Lower than usual precision floats are common in AI workloads as they take up less space and allow for higher throughput. For improved accuracy, the computation of the GEMM output will be done using full-precision floats, but the final storage will also be as a half-precision float.\nFor each kernel, I will explain the algorithm, how it interacts with the GPU architecture and memory hierarchy, and show the full code in CUDA C++. Finally, I will discuss the computational intensity of the kernel and benchmark its performance on the following NVIDIA GPUs: Tesla T4 (2017), Ampere A100-80GB (2020), Hopper H100 (2022), Hopper H200 (2023), and Blackwell B200 (2024).\n\n\n\nI will assume the reader has at least the following understanding of the CUDA programming model: as a programmer, when we ask the GPU to execute a CUDA kernel, we first decide on the dimensions of a grid of threads that will be launched. Threads are grouped into blocks, and multiple blocks are assigned to each Streaming Multiprocessor (SMs). A block is resident on an SM until all of its threads have completed, at which point it is evicted. Multiple blocks can be resident at a time on an SM, and the SM will execute whichever threads are ready for compute and not waiting for memory, in order to hide memory latency. GPUs have several types of memory: registers, shared memory, L1 and L2 caches, constant memory, and global memory. Using these types of memory effectively is crucial for maximizing performance. Performance can vary widely based on the specific architecture of the GPU, and we can understand how to improve a kernel by calculating its arithmetic intensity and checking whether the kernel is compute or memory bound.\nIf the reader is not familiar with these concepts, I refer them to Programming Massively Parallel Processors (Kirk & Hwu), an excellent resource that explains these concepts much better than I could in this post. Chapters 1 through 6 cover everything listed above."
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#naive-matrix-multiplication",
    "href": "posts/001-gemm-optimization/index.html#naive-matrix-multiplication",
    "title": "Note 001: GEMM Optimization",
    "section": "1. Naive Matrix Multiplication",
    "text": "1. Naive Matrix Multiplication\nIn a naive parallel computing model, we can have every thread be solely responsible for computing exactly one output element in the final matrix. Each thread would load the row from A and column from B that it needs for the dot product for that output element.\nHover over the numbered annotations for explanations of key parts.\n\nAnnotated Code\n#include &lt;cuda_fp16.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n__global__ void gemm_naive_kernel(const half* A, const half* B, half* C, \n                                  int M, int N, int K, \n1                                  float alpha, float beta) {\n    \n    // Calculate global row and column indices for this thread\n2    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    // Boundary check: ensure we don't access memory outside the matrix\n    if (row &lt; M && col &lt; N) {\n        float val = 0;\n        \n        // The K-loop: Perform the dot product\n        for (int i = 0; i &lt; K; i++) {\n3            val += __half2float(A[row * K + i]) * __half2float(B[i * N + col]);\n        }\n        \n        // Write result back to C\n        val = alpha * val + beta * __half2float(C[row * N + col]);\n        C[row * N + col] = __float2half(val);\n    }\n}\n\n// Wrapper function to be called from Host\n4extern \"C\" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n\n    dim3 block(16, 16);\n    // Grid calculation: ensures we cover the entire matrix (ceiling division)\n    dim3 grid(\n        (N + 15) / 16,\n        (M + 15) / 16\n    );\n\n    gemm_naive_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n    cudaDeviceSynchronize();\n}\n\n1\n\nhalf vs. float: We use half precision (FP16) for storage but perform accumulation in float (FP32). This is so that we can move data faster from global memory (only 2 bytes per element rather than 4), but during the accumulation computation, we don’t lose small updates due to the smaller mantissa in FP16. (For example, imagine adding 0.01 to a running sum of 1000: if our mantissa is small enough, we may significantly alter or even omit some updates.)\n\n2\n\n2D Indexing: Standard mapping of a 2D thread block to matrix coordinates. blockIdx tells us which tile we are in; threadIdx tells us the pixel within that tile.\n\n3\n\nThe Bottleneck: This line is the performance killer. For every single pixel in C, we are fetching the entire row of A and column of B from Global Memory (DRAM).\n\n4\n\nHost Wrapper: extern \"C\" prevents C++ name mangling, allowing us to easily call this function from ctypes (Python) or PyTorch extensions later in the project.\n\n\n\n\nRoofline Model\nFor each output element of C, we load K elements of A and K elements of B in order to compute a dot product. For each pair of elements in the dot product, we multiply them together and then add the result to the running sum. Therefore, for every 2 halves we load from global memory (a total of 4 bytes), we perform 2 floating point operations. So our computational intensity is 2 FLOPs divided by 4 bytes, or 0.5 FLOP/B.\nBelow, we can see the roofline model for our test suite of GPUs, compared to the computational intensity of our first kernel. This kernel will be memory-bound on every GPU. Our first course of action to improve the performance of our kernel should be to rethink our memory access pattern.\nDue to the hidden test suite, we technically don’t know the FLOPS throughput of our kernel on each GPU. However we have the runtime, so we can look at that as a proxy, and compare the computational intensity of the kernel to the ridge point of each GPU to see how close we are to saturating the memory bandwidth or not. This should be accurate for determining if we are compute or memory bound.\n\nBenchmark Targets: The Ridge Point represents the arithmetic intensity required to max out the GPU’s compute. Below this number, the kernel is memory bound.\n\n\n\n\n\n\n\n\n\nGPU Model\nMemory Bandwidth\nPeak FP16 Compute\nRidge Point (FLOP/Byte)\nRuntime (ms)\n\n\n\n\nNVIDIA T4\n320 GB/s\n65 TFLOPS\n203\n8.49\n\n\nNVIDIA A100 (80GB)\n2,039 GB/s\n312 TFLOPS\n153\n1.03\n\n\nNVIDIA H100 (SXM)\n3,350 GB/s\n989 TFLOPS\n295\n0.54\n\n\nNVIDIA H200 (SXM)\n4,800 GB/s\n989 TFLOPS\n206\n0.53\n\n\nNVIDIA B200\n8,000 GB/s\n2,500 TFLOPS\n312\n0.50"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#tiled-matrix-multiplication",
    "href": "posts/001-gemm-optimization/index.html#tiled-matrix-multiplication",
    "title": "Note 001: GEMM Optimization",
    "section": "2. Tiled Matrix Multiplication",
    "text": "2. Tiled Matrix Multiplication"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rohan Reddy / Notes",
    "section": "",
    "text": "Rohan Reddy is a software engineer based in New York City.\nRead more about me →"
  },
  {
    "objectID": "index.html#recent-notes",
    "href": "index.html#recent-notes",
    "title": "Rohan Reddy / Notes",
    "section": "Recent Notes",
    "text": "Recent Notes"
  }
]