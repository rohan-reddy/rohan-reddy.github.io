[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html",
    "href": "posts/001-gemm-optimization/index.html",
    "title": "Note 001: GEMM Optimization",
    "section": "",
    "text": "General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs.\n\n\nFormally, GEMM is defined as an operation on two input matrices \\(A\\) and \\(B\\), and an accumulation matrix \\(C\\), scaled by scalars \\(\\alpha\\) and \\(\\beta\\):\n\\[\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n\\]\nWhere:\n\n\\(A\\) is an \\(M \\times K\\) matrix.\n\\(B\\) is a \\(K \\times N\\) matrix.\n\\(C\\) is an \\(M \\times N\\) matrix.\n\n\nIn deep learning contexts, \\(\\beta\\) is often 0 (overwriting the output) or 1 (accumulating gradients), and \\(\\alpha\\) is typically 1.\n\n\n\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: \\(\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V\\). Aside from the softmax operation, everything else can be represented as GEMM:\n\nCalculating the scaled attention scores (\\(\\frac{Q \\times K^T}{\\sqrt{d}}\\)).\nCalculating the weighted sum of values (\\(\\text{scores} \\times V\\)).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale.\n\n\n\nAs I iterate on GEMM kernels, I will test them on the General Matrix Multiplication test suite and infrastructure on LeetGPU.com. As per the problem setup there, I will only be using native capabilities of the GPUs, so no libraries like CuTe or cuBLAS. The test suite is hidden, but the known constraints are that each of the matrix dimensions \\(M\\), \\(N\\), and \\(K\\) are between 16 and 4096. So the input matrices range from very small (a few hundred elements) to fairly large (16 million elements). The input matrices A and B are given as type half (half-precision floating point number). Lower than usual precision floats are common in AI workloads as they take up less space and allow for higher throughput. For improved accuracy, the computation of the GEMM output will be done using full-precision floats, but the final storage will also be as a half-precision float.\nFor each kernel, I will explain the algorithm, how it interacts with the GPU architecture and memory hierarchy, and show the full code in CUDA C++. Finally, I will discuss the arithmeticc intensity of the kernel and benchmark its performance on the following NVIDIA GPUs: Tesla T4 (2017), Ampere A100-80GB (2020), Hopper H100 (2022), Hopper H200 (2023), and Blackwell B200 (2024).\n\n\n\nI will assume the reader understands the basics of the CUDA programming model. If not, I recommend reading the first 6 chapters of Programming Massively Parallel Processors (Kirk & Hwu), an excellent resource and probably the canonical text on this topic."
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#introduction",
    "href": "posts/001-gemm-optimization/index.html#introduction",
    "title": "Note 001: GEMM Optimization",
    "section": "",
    "text": "General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs.\n\n\nFormally, GEMM is defined as an operation on two input matrices \\(A\\) and \\(B\\), and an accumulation matrix \\(C\\), scaled by scalars \\(\\alpha\\) and \\(\\beta\\):\n\\[\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n\\]\nWhere:\n\n\\(A\\) is an \\(M \\times K\\) matrix.\n\\(B\\) is a \\(K \\times N\\) matrix.\n\\(C\\) is an \\(M \\times N\\) matrix.\n\n\nIn deep learning contexts, \\(\\beta\\) is often 0 (overwriting the output) or 1 (accumulating gradients), and \\(\\alpha\\) is typically 1.\n\n\n\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: \\(\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V\\). Aside from the softmax operation, everything else can be represented as GEMM:\n\nCalculating the scaled attention scores (\\(\\frac{Q \\times K^T}{\\sqrt{d}}\\)).\nCalculating the weighted sum of values (\\(\\text{scores} \\times V\\)).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale.\n\n\n\nAs I iterate on GEMM kernels, I will test them on the General Matrix Multiplication test suite and infrastructure on LeetGPU.com. As per the problem setup there, I will only be using native capabilities of the GPUs, so no libraries like CuTe or cuBLAS. The test suite is hidden, but the known constraints are that each of the matrix dimensions \\(M\\), \\(N\\), and \\(K\\) are between 16 and 4096. So the input matrices range from very small (a few hundred elements) to fairly large (16 million elements). The input matrices A and B are given as type half (half-precision floating point number). Lower than usual precision floats are common in AI workloads as they take up less space and allow for higher throughput. For improved accuracy, the computation of the GEMM output will be done using full-precision floats, but the final storage will also be as a half-precision float.\nFor each kernel, I will explain the algorithm, how it interacts with the GPU architecture and memory hierarchy, and show the full code in CUDA C++. Finally, I will discuss the arithmeticc intensity of the kernel and benchmark its performance on the following NVIDIA GPUs: Tesla T4 (2017), Ampere A100-80GB (2020), Hopper H100 (2022), Hopper H200 (2023), and Blackwell B200 (2024).\n\n\n\nI will assume the reader understands the basics of the CUDA programming model. If not, I recommend reading the first 6 chapters of Programming Massively Parallel Processors (Kirk & Hwu), an excellent resource and probably the canonical text on this topic."
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#naive-matrix-multiplication",
    "href": "posts/001-gemm-optimization/index.html#naive-matrix-multiplication",
    "title": "Note 001: GEMM Optimization",
    "section": "1. Naive Matrix Multiplication",
    "text": "1. Naive Matrix Multiplication\nIn a naive parallel computing model, we can have every thread be solely responsible for computing exactly one output element in the final matrix. Each thread would load the row from A and column from B that it needs for the dot product for that output element.\nHover over the numbered annotations for explanations of key parts.\n\nAnnotated Code\n#include &lt;cuda_fp16.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n__global__ void gemm_naive_kernel(const half* A, const half* B, half* C, \n                                  int M, int N, int K, \n1                                  float alpha, float beta) {\n    \n    // Calculate global row and column indices for this thread\n2    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    // Boundary check: ensure we don't access memory outside the matrix\n    if (row &lt; M && col &lt; N) {\n        float val = 0;\n        \n        // The K-loop: Perform the dot product\n        for (int i = 0; i &lt; K; i++) {\n3            val += __half2float(A[row * K + i]) * __half2float(B[i * N + col]);\n        }\n        \n        // Write result back to C\n        val = alpha * val + beta * __half2float(C[row * N + col]);\n        C[row * N + col] = __float2half(val);\n    }\n}\n\n// Wrapper function to be called from Host\n4extern \"C\" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n\n    dim3 block(16, 16);\n    // Grid calculation: ensures we cover the entire matrix (ceiling division)\n    dim3 grid(\n        (N + 15) / 16,\n        (M + 15) / 16\n    );\n\n    gemm_naive_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n    cudaDeviceSynchronize();\n}\n\n1\n\nhalf vs. float: We use half precision (FP16) for storage but perform accumulation in float (FP32). This is so that we can move data faster from global memory (only 2 bytes per element rather than 4), but during the accumulation computation, we don’t lose small updates due to the smaller mantissa in FP16. (For example, imagine adding 0.01 to a running sum of 1000: if our mantissa is small enough, we may significantly alter or even omit some updates.)\n\n2\n\n2D Indexing: Standard mapping of a 2D thread block to matrix coordinates. blockIdx tells us which tile we are in; threadIdx tells us the pixel within that tile.\n\n3\n\nThe Bottleneck: This line is the performance killer. For every single pixel in C, we are fetching the entire row of A and column of B from Global Memory (DRAM).\n\n4\n\nHost Wrapper: extern \"C\" prevents C++ name mangling, allowing us to easily call this function from ctypes (Python) or PyTorch extensions later in the project.\n\n\n\n\nArithmetic Intensity\nFor each output element of C, we load K elements of A and K elements of B in order to compute a dot product. For each pair of elements in the dot product, we multiply them together and then add the result to the running sum. Therefore, for every 2 halves we load from global memory (a total of 4 bytes), we perform 2 floating point operations. So our computational intensity is 2 FLOPs divided by 4 bytes, or 0.5 FLOP/B.\n\n\nBenchmarks\nBelow, we can see the runtime of our kernel on the same test suite for each GPU. We can also compare the arithmetic intensity of the kernel to the ridge point of each GPU (the arithmetic intensity at which kernels switch from memory-bound to compute-bound). This kernel is highly memory-bound on every GPU. Our first course of action to improve the performance of our kernel should be to rethink our memory access pattern.\n\nIf our arithmetic intensity is below the Ridge Point, kernels are memory bound. Above the Ridge Point, kernels are compute bound.\n\n\n\n\n\n\n\n\n\nGPU Model\nMemory Bandwidth\nPeak FP16 Compute\nRidge Point (FLOP/Byte)\nRuntime (ms)\n\n\n\n\nNVIDIA T4\n320 GB/s\n65 TFLOPS\n203\n8.49\n\n\nNVIDIA A100 (80GB)\n2,039 GB/s\n312 TFLOPS\n153\n1.03\n\n\nNVIDIA H100 (SXM)\n3,350 GB/s\n989 TFLOPS\n295\n0.54\n\n\nNVIDIA H200 (SXM)\n4,800 GB/s\n989 TFLOPS\n206\n0.53\n\n\nNVIDIA B200\n8,000 GB/s\n2,500 TFLOPS\n312\n0.50"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#tiled-matrix-multiplication",
    "href": "posts/001-gemm-optimization/index.html#tiled-matrix-multiplication",
    "title": "Note 001: GEMM Optimization",
    "section": "2. Tiled Matrix Multiplication",
    "text": "2. Tiled Matrix Multiplication\nThe main issue with our memory access pattern above was that we are redundantly accessing each row N times and each column M times. Why? Recall that the output C is an M x N matrix. Therefore for \\(C_{1,1}\\), we need to compute the dot product of row 1 of A with column 1 of B; then for \\(C_{2,1}\\), we need to compute the dot product of row 2 of A with column 1 of B again. So we retrieve column 1 of B from global memory a total of M times. Similarly, row 1 of A is retrieved from global memroy a total of N times, since we access it once for each element in row 1 of the output.\n\n\n\nMemory hierarchy of an A100-40GB\n\n\nWhen we execute our kernel, we pass it a grid configuration that defines a total number of blocks and how we can index them, and a total number of threads per block and how we can index them. Multiple blocks will be assigned to a single Streaming Multiprocessor (SM) of the GPU at any given time. So all threads in an individual block have access to the same Shared Memory and L1 Cache on their resident Streaming Multiprocessor during execution. We can take advantage of this local memory to reduce our global memory accesses. This pattern is known as locality.\n\n\n\nVisualization of tiled matrix multiplication\n\n\nIn tiled matrix multiplication, we choose a tile size which will comprise the total threads in a single block. We will choose 16 x 16 as our tile size so that we have a nice total of 256 threads per block. (32 x 32 would also work, but beyond that we need to be cognizant of hardware restrictions on the maximum number of threads per block). We then loop over a wide row in A and a wide column in B, one tile at a time, as shown above. During each loop iteration, we have a single tile in A and tile in B to process. Each thread is responsible for loading in one element each from A and B to the block’s shared memory. Then in an inner loop, we compute the product of those tiles and add it to the running sum for the output tile. By the end of the outer loop, we have loaded in and processed all elements required for the final value of elements in the 16 x 16 output tile, and so we can write to global memory.\nOne additional optimization we introduce here is thread coarsening. This means that each thread is tasked with doing more work independently. The advantage of this approach is that if our grid ends up launching more total blocks than the hardware can assign to its SMs, then the blocks will inevitably be queued for assignment and execution. In that case, the blocks will be executed serially anyway, so we may as well have threads do more work in the first place and reduce some redundant data loading and synchronization overhead. However, we must be careful not to coarsen so much that we are no longer taking full advantage of the hardware. For our tiled matrix multiplication kernel, it can make sense for large matrices to have some coarsening. This is because although we have reduced redundancy in global memory accesses, we still will access the same “wide row” in A in two different blocks for two side-by-side output tiles in C. We can experiment with having a thread coarsening factor of 2, which means each block will process two output tiles in C rather than one.\n\nAnnotated Code\n#include &lt;cuda_fp16.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n#define TILE_WIDTH 16\n#define COARSE_FACTOR 2\n\n__global__ void gemm_tiled_kernel(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n    \n1    __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n\n    int row = TILE_WIDTH * blockIdx.y + threadIdx.y;\n2    int colStart = COARSE_FACTOR * TILE_WIDTH * blockIdx.x + threadIdx.x;\n\n    float sum[COARSE_FACTOR]; \n3    #pragma unroll\n    for (int c = 0; c &lt; COARSE_FACTOR; c++) {\n        sum[c] = 0.0f;\n    }\n\n    // Loop over the K-dimension (shared dimension)\n    for (int phase = 0; phase &lt; (K + TILE_WIDTH - 1) / TILE_WIDTH; phase++) {\n        \n        // --- Load A ---\n        // A is (M x K). \n        // Row comes from global 'row'. \n        // Col comes from 'phase' and 'threadIdx.x'.\n        int a_col = phase * TILE_WIDTH + threadIdx.x;\n        As[threadIdx.y][threadIdx.x] = \n            (row &lt; M && a_col &lt; K) ? \n4            __half2float(A[row * K + a_col]) : 0.0f;\n\n        #pragma unroll\n        for (int c = 0; c &lt; COARSE_FACTOR; c++) {\n            int col = colStart + c * TILE_WIDTH;\n\n            // --- Load B ---\n            // B is (K x N). \n            // Row comes from 'phase' and 'threadIdx.y'. \n            // Col comes from global 'col'.\n            int b_row = phase * TILE_WIDTH + threadIdx.y;\n            \n            Bs[threadIdx.y][threadIdx.x] = \n                (b_row &lt; K && col &lt; N) ?\n                __half2float(B[b_row * N + col]) : 0.0f; \n            \n5            __syncthreads();\n\n            for (int j = 0; j &lt; TILE_WIDTH; j++) {\n                sum[c] += As[threadIdx.y][j] * Bs[j][threadIdx.x];\n            }\n            __syncthreads();\n        }\n    }\n\n    #pragma unroll\n    for (int c = 0; c &lt; COARSE_FACTOR; c++) {\n        int col = colStart + c * TILE_WIDTH;\n6        if (row &lt; M && col &lt; N) {\n            int idx = row * N + col; // C is (M x N), stride is N\n            float initial_val = __half2float(C[idx]);\n            C[idx] = __float2half(alpha * sum[c] + beta * initial_val);\n        }\n    }\n}\n\nextern \"C\" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha,\n                      float beta) {\n\n    dim3 block(16, 16);\n    \n    dim3 grid((N + (TILE_WIDTH * COARSE_FACTOR) - 1) / (TILE_WIDTH * COARSE_FACTOR), \n7              (M + TILE_WIDTH - 1) / TILE_WIDTH);\n\n    gemm_tiled_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n    cudaDeviceSynchronize();\n}\n\n1\n\nShared memory: We declare our block shared memory. One can also dynamically pass the total size of block shared memory to the kernel at runtime if desired. In our case, we have a predetermined tile width. Note that we need to be cognizant of the total shared memory available on an SM. Our oldest GPU, the T4, has 64 KB of shared memory per SM. Here, we have two arrays of 16 x 16 floats each, so 512 total floats, so 4 KB. We’re well within the limits.\n\n2\n\nCoarsening: We set COARSE_FACTOR to 2, so each thread is going to load in 2 elements each from A and B, and compute 2 output elements in C. We are loading in two horizontal tiles at a time per block, so we need to apply our coarsening factor to our column computation.\n\n3\n\nLoop unrolling: #pragma unroll is a directive that asks the compiler to try to unroll the loop fully, especially if the total number of iterations is known at compile time. To unroll a loop means to duplicate the code in the loop body rather than perform a condition check and a jump back to the start of the loop body. This allows us to avoid the execution speed cost of checking the loop condition, with the tradeoff of increasing code size. From here on out, we will typically unroll any loop with a constant number of iterations.\n\n4\n\nBoundary checks: Our tiles are a fixed size. So if our matrix dimensions are not all multiples of 16, we will have some tiles that aren’t fully contained within the input matrices and try to access out-of-bound indices. We can simply set these values to 0 in shared memory so that they accumulate to 0 and don’t impact the result.\n\n5\n\n__syncthreads(): This instruction forces each thread in the block to halt here and wait until every other thread in the block reaches this point. This first syncthreads command is known as a Read-After-Write hazard, and the one after it is known as a Write-After-Read hazard. In the first case, individual threads rely on reading shared memory that other threads in their block are writing to. In the second case, if we don’t have a barrier, then some threads risk proceeding to the next loop iteration and modifying shared memory before other threads have read it for their computation on the previous iteration.\n\n6\n\nAnother boundary check: When we write to C, we again need to check that we are within bounds, since some tiles may not be fully contained at the end of the grid.\n\n7\n\nGrid calculation with coarsening: We adjust our grid calculation to account for the coarsening in the horizontal dimension; this impacts the total number of blocks we need horizontally.\n\n\n\n\nArithmetic Intensity\nNow that we are reusing some global memory, our arithmetic intensity is higher. The coarsening factor doesn’t impact the arithmetic intensity, so let’s ignore it for the calculation. A single thread is computing a single output element in C, but it doesn’t have to load every element in the vectors of A and B that are used for that dot product. It only has to load one element of A and one element of B per tile, and then it benefits from the other 15 elements it needs from each matrix for each tile that were loaded by other threads. Therefore we reduced the number of global memory accesses by a factor of 16. But we are performing the same number of floating point operations, so our arithmetic intensity is simply 16 times higher than that of the naive kernel. Hence the arithmetic intensity of this kernel is 8 FLOPS/B.\n\n\nBenchmarks\nThe runtime improved from our increase in arithmetic intensity. The kernel is still memory-bound though on every GPU. In the next section, we will address this by taking advantage of a fundamental hardware capability that happens to available in every GPU in our test set.\n\n\n\n\n\n\n\n\n\n\nGPU Model\nMemory Bandwidth\nPeak FP16 Compute\nRidge Point (FLOP/Byte)\nRuntime (ms)\n\n\n\n\nNVIDIA T4\n320 GB/s\n65 TFLOPS\n203\n6.73\n\n\nNVIDIA A100 (80GB)\n2,039 GB/s\n312 TFLOPS\n153\n0.72\n\n\nNVIDIA H100 (SXM)\n3,350 GB/s\n989 TFLOPS\n295\n0.37\n\n\nNVIDIA H200 (SXM)\n4,800 GB/s\n989 TFLOPS\n206\n0.36\n\n\nNVIDIA B200\n8,000 GB/s\n2,500 TFLOPS\n312\n0.33"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#warp-matrix-multiply-accumulate",
    "href": "posts/001-gemm-optimization/index.html#warp-matrix-multiply-accumulate",
    "title": "Note 001: GEMM Optimization",
    "section": "3. Warp Matrix Multiply Accumulate",
    "text": "3. Warp Matrix Multiply Accumulate\n\nAnnotated Code\n\n\nArithmetic Intensity\n\n\nBenchmarks"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#ping-pong-buffer",
    "href": "posts/001-gemm-optimization/index.html#ping-pong-buffer",
    "title": "Note 001: GEMM Optimization",
    "section": "4. Ping-Pong Buffer",
    "text": "4. Ping-Pong Buffer\n\nAnnotated Code\n\n\nArithmetic Intensity\n\n\nBenchmarks"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#swizzling",
    "href": "posts/001-gemm-optimization/index.html#swizzling",
    "title": "Note 001: GEMM Optimization",
    "section": "5. Swizzling",
    "text": "5. Swizzling\n\nAnnotated Code\n\n\nArithmetic Intensity\n\n\nBenchmarks"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#arbitrary-matrix-dimensions",
    "href": "posts/001-gemm-optimization/index.html#arbitrary-matrix-dimensions",
    "title": "Note 001: GEMM Optimization",
    "section": "6. Arbitrary Matrix Dimensions",
    "text": "6. Arbitrary Matrix Dimensions\n\nAnnotated Code\n\n\nArithmetic Intensity\n\n\nBenchmarks"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#further-optimizations",
    "href": "posts/001-gemm-optimization/index.html#further-optimizations",
    "title": "Note 001: GEMM Optimization",
    "section": "Further Optimizations",
    "text": "Further Optimizations"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#additional-reading",
    "href": "posts/001-gemm-optimization/index.html#additional-reading",
    "title": "Note 001: GEMM Optimization",
    "section": "Additional Reading",
    "text": "Additional Reading"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rohan Reddy / Notes",
    "section": "",
    "text": "Rohan Reddy is a software engineer based in New York City.\nRead more about me →"
  },
  {
    "objectID": "index.html#recent-notes",
    "href": "index.html#recent-notes",
    "title": "Rohan Reddy / Notes",
    "section": "Recent Notes",
    "text": "Recent Notes"
  }
]