[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About me\nI grew up in Chapel Hill, North Carolina. I originally moved to New York over a decade ago to attend college at NYU, where I studied finance and computer science, and I’ve pretty much been here ever since. After graduation, I worked at Amazon for a little over 4 years in their demand forecasting group, where I worked on both production machine learning systems and research projects. In particular, I contributed to research on hierarchical forecasting that my team presented at NeurIPS in 2021 and subsequently published in the International Journal of Forecasting. In 2023, I took a sabbatical to explore my interests, which included math and graphics rendering. The following year, I ended up enrolling in a second bachelor’s degree in mathematics from Indiana University East, which I recently completed (December 2025). Now, I’m studying all things GPU-related and keeping an eye out for roles that are aligned with my interests. If you have something that could be a good fit, please feel free to reach out to me on LinkedIn or directly at rreddy.nyc@gmail.com.\nApart from work, I like to take photos whenever I get the chance to travel somewhere interesting. I try to experiment and get a little bit better each time. In particular, I photograph a lot of animals and landscapes. If you’re interested, you can check some of them out at my photography site. I also enjoy playing tennis, swimming, lifting, playing the piano, and baking.\n\n\nAbout this blog\nInitially, I’m focusing on learning as much as possible about GPU architecture from a software engineer’s perspective, and optimizing performance of GPU kernels for common AI workloads. Next, I’m thinking of exploring topics like distributed communication across multiple GPUs (e.g. NCCL), low-precision quantization, and compiler-level abstractions (e.g. Triton, MLIR). If you have suggestions on learning/research directions or any feedback for me, I love to hear them - please email me directly at rreddy.nyc@gmail.com."
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html",
    "href": "posts/001-gemm-optimization/index.html",
    "title": "Note 001: GEMM Optimization",
    "section": "",
    "text": "General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs.\n\n\nFormally, GEMM is defined as an operation on two input matrices \\(A\\) and \\(B\\), and an accumulation matrix \\(C\\), scaled by scalars \\(\\alpha\\) and \\(\\beta\\):\n\\[\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n\\]\nWhere:\n\n\\(A\\) is an \\(M \\times K\\) matrix.\n\\(B\\) is a \\(K \\times N\\) matrix.\n\\(C\\) is an \\(M \\times N\\) matrix.\n\n\nIn deep learning contexts, \\(\\beta\\) is often 0 (overwriting the output) or 1 (accumulating gradients), and \\(\\alpha\\) is typically 1.\n\n\n\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: \\(\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V\\). Aside from the softmax operation, everything else can be represented as GEMM:\n\nCalculating the scaled attention scores (\\(\\frac{Q \\times K^T}{\\sqrt{d}}\\)).\nCalculating the weighted sum of values (\\(\\text{scores} \\times V\\)).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale.\n\n\n\nAs I iterate on GEMM kernels, I will test them on the General Matrix Multiplication test suite and infrastructure on LeetGPU.com. As per the problem setup there, I will only be using native capabilities of the GPUs, so no libraries like CuTe or cuBLAS. The test suite is hidden, but the known constraints are that each of the matrix dimensions \\(M\\), \\(N\\), and \\(K\\) are between 16 and 4096. So the input matrices range from very small (a few hundred elements) to fairly large (16 million elements). The input matrices A and B are given as type half (half-precision floating point number). Lower than usual precision floats are common in AI workloads as they take up less space and allow for higher throughput. For improved accuracy, the computation of the GEMM output will be done using full-precision floats, but the final storage will also be as a half-precision float.\nFor each kernel, I will explain the algorithm, how it interacts with the GPU architecture and memory hierarchy, and show the full code in CUDA C++. Finally, I will discuss the arithmetic intensity of the kernel and benchmark its performance on the following NVIDIA GPUs: Tesla T4 (2017), Ampere A100-80GB (2020), Hopper H100 (2022), Hopper H200 (2023), and Blackwell B200 (2024).\n\n\n\nI will assume the reader understands the basics of the CUDA programming model. If not, I recommend reading the first 6 chapters of Programming Massively Parallel Processors (Kirk & Hwu), an excellent resource and probably the canonical text on this topic."
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#introduction",
    "href": "posts/001-gemm-optimization/index.html#introduction",
    "title": "Note 001: GEMM Optimization",
    "section": "",
    "text": "General Matrix Multiply, or GEMM, is a linear algebra operation that comprises the majority of computing done by modern deep learning models. In this note, I will explain how we can iteratively optimize GEMM implementations in CUDA until we have almost saturated the capability of modern NVIDIA GPUs.\n\n\nFormally, GEMM is defined as an operation on two input matrices \\(A\\) and \\(B\\), and an accumulation matrix \\(C\\), scaled by scalars \\(\\alpha\\) and \\(\\beta\\):\n\\[\nC = \\alpha \\cdot (A \\times B) + \\beta \\cdot C\n\\]\nWhere:\n\n\\(A\\) is an \\(M \\times K\\) matrix.\n\\(B\\) is a \\(K \\times N\\) matrix.\n\\(C\\) is an \\(M \\times N\\) matrix.\n\n\nIn deep learning contexts, \\(\\beta\\) is often 0 (overwriting the output) or 1 (accumulating gradients), and \\(\\alpha\\) is typically 1.\n\n\n\nIn modern Transformer architectures, GEMM operations account for the vast majority of total Floating Point Operations (FLOPs). This is due to the structure of the Attention operation: \\(\\text{softmax}(\\frac{Q \\times K^T}{\\sqrt{d}}) \\times V\\). Aside from the softmax operation, everything else can be represented as GEMM:\n\nCalculating the scaled attention scores (\\(\\frac{Q \\times K^T}{\\sqrt{d}}\\)).\nCalculating the weighted sum of values (\\(\\text{scores} \\times V\\)).\n\nSince GEMM dominates the runtime, even a small percentage improvement in kernel efficiency can realize massive savings in training and inference costs at scale.\n\n\n\nAs I iterate on GEMM kernels, I will test them on the General Matrix Multiplication test suite and infrastructure on LeetGPU.com. As per the problem setup there, I will only be using native capabilities of the GPUs, so no libraries like CuTe or cuBLAS. The test suite is hidden, but the known constraints are that each of the matrix dimensions \\(M\\), \\(N\\), and \\(K\\) are between 16 and 4096. So the input matrices range from very small (a few hundred elements) to fairly large (16 million elements). The input matrices A and B are given as type half (half-precision floating point number). Lower than usual precision floats are common in AI workloads as they take up less space and allow for higher throughput. For improved accuracy, the computation of the GEMM output will be done using full-precision floats, but the final storage will also be as a half-precision float.\nFor each kernel, I will explain the algorithm, how it interacts with the GPU architecture and memory hierarchy, and show the full code in CUDA C++. Finally, I will discuss the arithmetic intensity of the kernel and benchmark its performance on the following NVIDIA GPUs: Tesla T4 (2017), Ampere A100-80GB (2020), Hopper H100 (2022), Hopper H200 (2023), and Blackwell B200 (2024).\n\n\n\nI will assume the reader understands the basics of the CUDA programming model. If not, I recommend reading the first 6 chapters of Programming Massively Parallel Processors (Kirk & Hwu), an excellent resource and probably the canonical text on this topic."
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#naive-matrix-multiplication",
    "href": "posts/001-gemm-optimization/index.html#naive-matrix-multiplication",
    "title": "Note 001: GEMM Optimization",
    "section": "1. Naive Matrix Multiplication",
    "text": "1. Naive Matrix Multiplication\nIn a naive parallel computing model, we can have every thread be solely responsible for computing exactly one output element in the final matrix. Each thread would load the row from A and column from B that it needs for the dot product for that output element.\nHover over the numbered annotations for explanations of key parts.\n\nAnnotated Code\n#include &lt;cuda_fp16.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n__global__ void gemm_naive_kernel(const half* A, const half* B, half* C, \n                                  int M, int N, int K, \n1                                  float alpha, float beta) {\n    \n    // Calculate global row and column indices for this thread\n2    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    \n    // Boundary check: ensure we don't access memory outside the matrix\n    if (row &lt; M && col &lt; N) {\n        float val = 0;\n        \n        // The K-loop: Perform the dot product\n        for (int i = 0; i &lt; K; i++) {\n3            val += __half2float(A[row * K + i]) * __half2float(B[i * N + col]);\n        }\n        \n        // Write result back to C\n        val = alpha * val + beta * __half2float(C[row * N + col]);\n        C[row * N + col] = __float2half(val);\n    }\n}\n\n// Wrapper function to be called from Host\n4extern \"C\" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n\n    dim3 block(16, 16);\n    // Grid calculation: ensures we cover the entire matrix (ceiling division)\n    dim3 grid(\n        (N + 15) / 16,\n        (M + 15) / 16\n    );\n\n    gemm_naive_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n    cudaDeviceSynchronize();\n}\n\n1\n\nhalf vs. float: We use half precision (FP16) for storage but perform accumulation in float (FP32). This is so that we can move data faster from global memory (only 2 bytes per element rather than 4), but during the accumulation computation, we don’t lose small updates due to the smaller mantissa in FP16. (For example, imagine adding 0.01 to a running sum of 1000: if our mantissa is small enough, we may significantly alter or even omit some updates.)\n\n2\n\n2D Indexing: Standard mapping of a 2D thread block to matrix coordinates. blockIdx tells us which tile we are in; threadIdx tells us the pixel within that tile.\n\n3\n\nThe Bottleneck: This line is the performance killer. For every single pixel in C, we are fetching the entire row of A and column of B from Global Memory (DRAM).\n\n4\n\nHost Wrapper: extern \"C\" prevents C++ name mangling, allowing us to easily call this function from ctypes (Python) or PyTorch extensions later in the project.\n\n\n\n\nArithmetic Intensity\nFor each output element of C, we load K elements of A and K elements of B in order to compute a dot product. For each pair of elements in the dot product, we multiply them together and then add the result to the running sum. Therefore, for every 2 halves we load from global memory (a total of 4 bytes), we perform 2 floating point operations. So our computational intensity is 2 FLOPs divided by 4 bytes, or 0.5 FLOP/B.\n\n\nBenchmarks\nBelow, we can see the runtime of our kernel on the same test suite for each GPU. We can also compare the arithmetic intensity of the kernel to the ridge point of each GPU (the arithmetic intensity at which kernels switch from memory-bound to compute-bound). This kernel is highly memory-bound on every GPU. Our first course of action to improve the performance of our kernel should be to rethink our memory access pattern.\n\nIf our arithmetic intensity is below the Ridge Point, kernels are memory bound. Above the Ridge Point, kernels are compute bound.\n\n\n\n\n\n\n\n\n\nGPU Model\nMemory Bandwidth\nPeak FP16 Compute\nRidge Point (FLOP/Byte)\nRuntime (ms)\n\n\n\n\nNVIDIA T4\n320 GB/s\n65 TFLOPS\n203\n8.49\n\n\nNVIDIA A100 (80GB)\n2,039 GB/s\n312 TFLOPS\n153\n1.03\n\n\nNVIDIA H100 (SXM)\n3,350 GB/s\n989 TFLOPS\n295\n0.54\n\n\nNVIDIA H200 (SXM)\n4,800 GB/s\n989 TFLOPS\n206\n0.53\n\n\nNVIDIA B200\n8,000 GB/s\n2,500 TFLOPS\n312\n0.50"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#tiled-matrix-multiplication",
    "href": "posts/001-gemm-optimization/index.html#tiled-matrix-multiplication",
    "title": "Note 001: GEMM Optimization",
    "section": "2. Tiled Matrix Multiplication",
    "text": "2. Tiled Matrix Multiplication\nThe main issue with our memory access pattern above was that we are redundantly accessing each row N times and each column M times. Why? Recall that the output C is an M x N matrix. Therefore for \\(C_{1,1}\\), we need to compute the dot product of row 1 of A with column 1 of B; then for \\(C_{2,1}\\), we need to compute the dot product of row 2 of A with column 1 of B again. So we retrieve column 1 of B from global memory a total of M times. Similarly, row 1 of A is retrieved from global memory a total of N times, since we access it once for each element in row 1 of the output.\n\n\n\nMemory hierarchy of an A100-40GB\n\n\nWhen we execute our kernel, we pass it a grid configuration that defines a total number of blocks and how we can index them, and a total number of threads per block and how we can index them. Multiple blocks will be assigned to a single Streaming Multiprocessor (SM) of the GPU at any given time. So all threads in an individual block have access to the same Shared Memory and L1 Cache on their resident Streaming Multiprocessor during execution. We can take advantage of this local memory to reduce our global memory accesses. This pattern is known as locality.\n\n\n\nVisualization of tiled matrix multiplication\n\n\nIn tiled matrix multiplication, we choose a tile size which will comprise the total threads in a single block. We will choose 16 x 16 as our tile size so that we have a nice total of 256 threads per block. (32 x 32 would also work, but beyond that we need to be cognizant of hardware restrictions on the maximum number of threads per block). We then loop over a wide row in A and a wide column in B, one tile at a time, as shown above. During each loop iteration, we have a single tile in A and tile in B to process. Each thread is responsible for loading in one element each from A and B to the block’s shared memory. Then in an inner loop, we compute the product of those tiles and add it to the running sum for the output tile. By the end of the outer loop, we have loaded in and processed all elements required for the final value of elements in the 16 x 16 output tile, and so we can write to global memory.\nOne additional optimization we introduce here is thread coarsening. This means that each thread is tasked with doing more work independently. The advantage of this approach is that if our grid ends up launching more total blocks than the hardware can assign to its SMs, then the blocks will inevitably be queued for assignment and execution. In that case, the blocks will be executed serially anyway, so we may as well have threads do more work in the first place and reduce some redundant data loading and synchronization overhead. However, we must be careful not to coarsen so much that we are no longer taking full advantage of the hardware. For our tiled matrix multiplication kernel, it can make sense for large matrices to have some coarsening. This is because although we have reduced redundancy in global memory accesses, we still will access the same “wide row” in A in two different blocks for two side-by-side output tiles in C. We can experiment with having a thread coarsening factor of 2, which means each block will process two output tiles in C rather than one.\n\nAnnotated Code\n#include &lt;cuda_fp16.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n#define TILE_WIDTH 16\n#define COARSE_FACTOR 2\n\n__global__ void gemm_tiled_kernel(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n    \n1    __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n\n    int row = TILE_WIDTH * blockIdx.y + threadIdx.y;\n2    int colStart = COARSE_FACTOR * TILE_WIDTH * blockIdx.x + threadIdx.x;\n\n    float sum[COARSE_FACTOR]; \n3    #pragma unroll\n    for (int c = 0; c &lt; COARSE_FACTOR; c++) {\n        sum[c] = 0.0f;\n    }\n\n    // Loop over the K-dimension (shared dimension)\n    for (int phase = 0; phase &lt; (K + TILE_WIDTH - 1) / TILE_WIDTH; phase++) {\n        \n        // --- Load A ---\n        // A is (M x K). \n        // Row comes from global 'row'. \n        // Col comes from 'phase' and 'threadIdx.x'.\n        int a_col = phase * TILE_WIDTH + threadIdx.x;\n        As[threadIdx.y][threadIdx.x] = \n            (row &lt; M && a_col &lt; K) ? \n4            __half2float(A[row * K + a_col]) : 0.0f;\n\n        #pragma unroll\n        for (int c = 0; c &lt; COARSE_FACTOR; c++) {\n            int col = colStart + c * TILE_WIDTH;\n\n            // --- Load B ---\n            // B is (K x N). \n            // Row comes from 'phase' and 'threadIdx.y'. \n            // Col comes from global 'col'.\n            int b_row = phase * TILE_WIDTH + threadIdx.y;\n            \n            Bs[threadIdx.y][threadIdx.x] = \n                (b_row &lt; K && col &lt; N) ?\n                __half2float(B[b_row * N + col]) : 0.0f; \n            \n5            __syncthreads();\n\n            for (int j = 0; j &lt; TILE_WIDTH; j++) {\n                sum[c] += As[threadIdx.y][j] * Bs[j][threadIdx.x];\n            }\n            __syncthreads();\n        }\n    }\n\n    #pragma unroll\n    for (int c = 0; c &lt; COARSE_FACTOR; c++) {\n        int col = colStart + c * TILE_WIDTH;\n6        if (row &lt; M && col &lt; N) {\n            int idx = row * N + col; // C is (M x N), stride is N\n            float initial_val = __half2float(C[idx]);\n            C[idx] = __float2half(alpha * sum[c] + beta * initial_val);\n        }\n    }\n}\n\nextern \"C\" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha,\n                      float beta) {\n\n    dim3 block(TILE_WIDTH, TILE_WIDTH);\n    \n    dim3 grid(\n7        (N + (TILE_WIDTH * COARSE_FACTOR) - 1) / (TILE_WIDTH * COARSE_FACTOR),\n        (M + TILE_WIDTH - 1) / TILE_WIDTH\n    ); \n\n    gemm_tiled_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n    cudaDeviceSynchronize();\n}\n\n1\n\nShared memory: We declare our block shared memory. One can also dynamically pass the total size of block shared memory to the kernel at runtime if desired. In our case, we have a predetermined tile width. Note that we need to be cognizant of the total shared memory available on an SM. Our oldest GPU, the T4, has 64 KB of shared memory per SM. Here, we have two arrays of 16 x 16 floats each, so 512 total floats, so 4 KB. We’re well within the limits.\n\n2\n\nCoarsening: We set COARSE_FACTOR to 2, so each thread is going to load in 2 elements each from A and B, and compute 2 output elements in C. We are loading in two horizontal tiles at a time per block, so we need to apply our coarsening factor to our column computation.\n\n3\n\nLoop unrolling: #pragma unroll is a directive that asks the compiler to try to unroll the loop fully, especially if the total number of iterations is known at compile time. To unroll a loop means to duplicate the code in the loop body rather than perform a condition check and a jump back to the start of the loop body. This allows us to avoid the execution speed cost of checking the loop condition, with the tradeoff of increasing code size. From here on out, we will typically unroll any loop with a constant number of iterations.\n\n4\n\nBoundary checks: Our tiles are a fixed size. So if our matrix dimensions are not all multiples of 16, we will have some tiles that aren’t fully contained within the input matrices and try to access out-of-bound indices. We can simply set these values to 0 in shared memory so that they accumulate to 0 and don’t impact the result.\n\n5\n\n__syncthreads(): This instruction forces each thread in the block to halt here and wait until every other thread in the block reaches this point. This first syncthreads command is known as a Read-After-Write hazard, and the one after it is known as a Write-After-Read hazard. In the first case, individual threads rely on reading shared memory that other threads in their block are writing to. In the second case, if we don’t have a barrier, then some threads risk proceeding to the next loop iteration and modifying shared memory before other threads have read it for their computation on the previous iteration.\n\n6\n\nAnother boundary check: When we write to C, we again need to check that we are within bounds, since some tiles may not be fully contained at the end of the grid.\n\n7\n\nGrid calculation with coarsening: We adjust our grid calculation to account for the coarsening in the horizontal dimension; this impacts the total number of blocks we need horizontally.\n\n\n\n\nArithmetic Intensity\nNow that we are reusing some global memory, our arithmetic intensity is higher. The coarsening factor doesn’t impact the arithmetic intensity, so let’s ignore it for the calculation. A single thread is computing a single output element in C, but it doesn’t have to load every element in the vectors of A and B that are used for that dot product. It only has to load one element of A and one element of B per tile, and then it benefits from the other 15 elements it needs from each matrix for each tile that were loaded by other threads. Therefore we reduced the number of global memory accesses by a factor of 16. But we are performing the same number of floating point operations, so our arithmetic intensity is simply 16 times higher than that of the naive kernel. Hence the arithmetic intensity of this kernel is 8 FLOPs/B.\n\n\nBenchmarks\nThe runtime improved from our increase in arithmetic intensity. The kernel is still memory-bound though on every GPU. In the next section, we will address this by taking advantage of a fundamental hardware capability that happens to available in every GPU in our test set.\n\n\n\n\n\n\n\n\n\n\nGPU Model\nMemory Bandwidth\nPeak FP16 Compute\nRidge Point (FLOP/Byte)\nRuntime (ms)\n\n\n\n\nNVIDIA T4\n320 GB/s\n65 TFLOPS\n203\n6.73\n\n\nNVIDIA A100 (80GB)\n2,039 GB/s\n312 TFLOPS\n153\n0.72\n\n\nNVIDIA H100 (SXM)\n3,350 GB/s\n989 TFLOPS\n295\n0.37\n\n\nNVIDIA H200 (SXM)\n4,800 GB/s\n989 TFLOPS\n206\n0.36\n\n\nNVIDIA B200\n8,000 GB/s\n2,500 TFLOPS\n312\n0.33"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#warp-matrix-multiply-accumulate",
    "href": "posts/001-gemm-optimization/index.html#warp-matrix-multiply-accumulate",
    "title": "Note 001: GEMM Optimization",
    "section": "3. Warp Matrix Multiply Accumulate",
    "text": "3. Warp Matrix Multiply Accumulate\nEvery GPU in our test suite is modern enough to be equipped with Tensor Cores: programmable matrix-multiply-and-accumulate units that deliver massively higher throughput. Each SM has many of these Tensor Cores. An individual Tensor Core performs the operation \\(D = A \\times B + C\\), where every matrix in the operation has size 4x4. We call the shape of this operation 4x4x4. Additionally, Tensor Cores natively handle mixed-precision: the input matrices A and B are expected to be half-precision (FP16), while the accumulators C and D can be either FP16 or FP32.\n\n\n\nTensor Core performing a 4x4x4 matrix multiply and accumulate operation\n\n\nThis capability is exposed to us as the Warp Matrix Multiply Accumulate API (WMMA). During program execution, a full warp of execution will use multiple Tensor Cores at a time in order to process a 16x16x16 MMA operation.\nThere are several advantages of using WMMA rather than manually programming the matrix multiply and accumulate operation like we did in previous kernels.\n\nSingle instruction: As opposed to issuing separate multiplication and addition instructions manually, the warp scheduler issues a single instruction to the Tensor Core hardware, which proceeds to take over the rest of the operation. GPUs have a limited rate at which they can feed instructions to the execution units, so this allows us to issue memory requests much faster and get closer to saturating the memory bus.\nMatrix loading: The load_matrix_sync instruction in WMMA is optimized to use 128-bit global loads. So it retrieves 16 bytes (8 halves) in a single transaction. Meanwhile, when we manually load half data, we are loading 2 bytes at a time unless we specify otherwise (discussed in a subsequent section, when we explicitly issue vectorized loads).\nDedicated registers: Tensor Cores have dedicated register file data paths and accumulation buffers, laid out to maximize efficiency. We don’t have to deal with register pressure (when we risk allocating too many local variables that live in registers, which can spill over to slower memory stores in we exceed the register capacity) or bank conflicts (discussed in a subsequent section). We don’t have to manage all of this ourselves as it’s already fully optimized when we use the Tensor Cores.\n\nOne disadvantage of WMMA is that we are locked into the 16x16x16 operation shape. Later on, we’ll adapt our kernel to handle any arbitrary matrix sizes. For now, we’ll have our host code decide whether to use our WMMA kernel based on the input matrix sizes.\n\nAnnotated Code\n#include &lt;cuda_runtime.h&gt;\n#include &lt;cuda_fp16.h&gt;\n\n#include &lt;mma.h&gt;\n\nusing namespace nvcuda;\n\n#define WARP_SIZE 32\n\n__global__ void gemm_wmma(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n   \n   // Leading dimensions for Row-Major matrices\n   int lead_dim_A = K; // A: M x K. Stride between rows is K\n   int lead_dim_B = N; // B: K x N. Stride between rows is N\n   int lead_dim_C = N; // C: M x N. Stride between rows is N\n\n   // 2D grid tiling. We will have multiple warps worth of threads in the x dimension.\n   // Hence warp_col is divided by warp size. \n   int warp_row = blockDim.y * blockIdx.y + threadIdx.y;\n   int warp_col = (blockDim.x * blockIdx.x + threadIdx.x) / WARP_SIZE;\n\n    // Declare fragments\n1    wmma::fragment&lt;wmma::matrix_a, 16, 16, 16, half, wmma::row_major&gt; A_frag;\n    wmma::fragment&lt;wmma::matrix_b, 16, 16, 16, half, wmma::row_major&gt; B_frag;\n    wmma::fragment&lt;wmma::accumulator, 16, 16, 16, float&gt; accum_frag;\n    wmma::fragment&lt;wmma::accumulator, 16, 16, 16, half&gt; C_frag;\n\n    // Initialize the accumulator fragment for A * B with zeroes.\n    wmma::fill_fragment(accum_frag, 0.0f);\n\n2    for (int i = 0; i &lt; K; i += 16) {\n        // Get the starting row and column of our 16 x 16 tiles in both A and B.\n        int row_A = warp_row * 16;\n        int col_A = i;\n        int row_B = i;\n        int col_B = warp_col * 16;\n\n        // Check bounds\n        if (row_A &lt; M && col_A &lt; K && row_B &lt; K && col_B &lt; N) {\n\n            // Load matrices. \n3            wmma::load_matrix_sync(A_frag, A + row_A * lead_dim_A + col_A, lead_dim_A);\n            wmma::load_matrix_sync(B_frag, B + row_B * lead_dim_B + col_B, lead_dim_B);\n\n            // Perform MMA. \n4            wmma::mma_sync(accum_frag, A_frag, B_frag, accum_frag);\n        }\n    }\n\n    int row_C = warp_row * 16;\n    int col_C = warp_col * 16;\n\n    // Complete the GEMM operation: scale and add result fragments, then write to global memory\n    if (row_C &lt; M && col_C &lt; N) {\n        wmma::load_matrix_sync(C_frag, C + row_C * lead_dim_C + col_C, lead_dim_C, wmma::mem_row_major);\n\n5        for (int i = 0; i &lt; C_frag.num_elements; i++) {\n            C_frag.x[i] = __float2half(alpha * accum_frag.x[i] + beta * __half2float(C_frag.x[i]));\n        }\n\n        // Store the result in global memory\n6        wmma::store_matrix_sync(C + row_C * lead_dim_C + col_C, C_frag, lead_dim_C, wmma::mem_row_major);\n    }\n}\n\n// Same as in Tiled Matrix Multiplication\n__global__ void gemm_tiled_kernel(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) { ... }\n\n\nextern \"C\" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n\n7    if (M % 16 == 0 && N % 16 == 0 && K % 16 == 0) {\n        const int WARPS_X = 4, WARPS_Y = 4;\n8        dim3 blockDim(WARPS_X * WARP_SIZE, WARPS_Y);\n        \n        int num_col_tiles = N / 16;\n        int num_row_tiles = M / 16;\n        \n        dim3 gridDim(\n            (num_col_tiles + WARPS_X - 1) / WARPS_X,\n            (num_row_tiles + WARPS_Y - 1) / WARPS_Y\n        );\n        \n        gemm_wmma&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n    } else {\n        dim3 block(TILE_WIDTH, TILE_WIDTH);\n        dim3 grid(\n            (N + (TILE_WIDTH * COARSE_FACTOR) - 1) / (TILE_WIDTH * COARSE_FACTOR), \n            (M + TILE_WIDTH - 1) / TILE_WIDTH\n        );\n\n        gemm_tiled_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n    }\n}\n\n1\n\nFragments: The operand matrices must be represented in the registers of Tensor Cores before MMA is performed. Since MMA is a warp-wide operation, these registers are distributed between the threads of a warp. Each thread holds a fragment of the overall matrix. A fragment is a templated type that accepts parameters for: the matrix the fragment holds, the shape of the overall operation, the data type, and whether the data is row or column major for the operand matrices. We pass in 16 three times for the shape of the overall operation to represent that the number of rows the fragment stores, the number of columns the fragment stores, and the dot product length are all 16.\n\n2\n\nThe K-loop: Each warp computes one 16 x 16 tile of A * B. We loop over rows of A and columns of B. Each row of A and column of B has K elements. Overall, we are computing a 16 x 16 output tile in C: C (16 x 16) = A (16 x K) * B(K x 16). However, we can only store and use 16 x 16 chunks of A and B at once for the MMA operation. Therefore we need to split K into chunks of 16. On each loop iteration, we accumulate C (16 x 16) += A (16 x 16) * B (16 x 16).\n\n3\n\nLoading into a fragment: To load data into a fragment, we need to specify the fragment to load into, the pointer to the memory we are loading from, and the leading dimension of the matrix (so that the operation knows the stride length between rows for a row-major matrix, or between columns for a column-major matrix).\n\n4\n\nMatrix Multiply Accumulate: Computes Arg1 = Arg2 * Arg3 + Arg4.\n\n5\n\nModifying data within fragments: There are 16 x 16 = 256 elements in C_frag and 32 threads per warp. Each thread therefore holds 256 / 32 = 8 elements. So the loop will have 8 iterations. The fragment’s internal storage is opaque - we don’t know which thread holds each element. Luckily, this doesn’t matter for element-wise operations like scaling. What about for accum_frag and C_frag? As they are declared with identical template parameters, they are guaranteed to have the same internal layout. Hence we can be sure we are adding the correct corresponding elements.\n\n6\n\nStoring back to global memory: Here we need to pass the pointer to memory that we are storing into, the fragment we are loading from, the leading dimension of the matrix, and whether the matrix is row or column major.\n\n7\n\nRestrictions on WMMA: WMMA strictly handles 16x16x16 operations only, so we need to check that our matrix dimensions are multiples of 16. If not, we’ll launch our tiled GEMM kernel. In a later section, we will adjust our WMMA kernel to handle arbitrary matrix dimensions.\n\n8\n\nGrid Dimensions: This works out to be (128, 4), so we have 512 total threads per block. Each row in our block has 128 threads, so a total of 4 warps, and then we have 4 rows, so we essentially have a 4x4 grid of warps in each block. Since each warp computes a 16x16 output tile, each warp is handling the same output as each block did in our tiled GEMM kernel. Since each block has a 4x4 grid of warps, we are then computing a 64x64 output tile of C for each block. We know that our matrix dimensions are divisible by 16, but they may not be divisible by 64. So at the blocks at the edge of our grid, we may have some warps that fall out of bounds of C. Luckily we have the necessary boundary checks in our kernel, so we just need to do our ceiling division here to ensure our blocks fully cover C, without worrying about if some of them go beyond the edges of C.\n\n\n\n\nArithmetic Intensity\nTo calculate the arithmetic intensity of this kernel, we will focus on the main loop where the loading from global memory and MMA operations happen. On each loop iteration, a warp collectively loads one 16x16 tile from each of A and B. So we retrieve 512 half-precision floats for a total of 1024 bytes. Then we are modifying the running sums for a 16x16 output tile in C. For each pixel in this output tile, we are taking a dot product of two 16-element vectors, so we perform 16 multiplications and 16 additions. Therefore we perform 32 FLOPs for each pixel in the 16x16 output tile, for a total of 8192 FLOPs. Therefore, our arithmetic intensity is approximately 8192 / 1024 = 8 FLOPs/B.\nNotice that this is exactly the same as the arithmetic intensity of our previous tiled matrix multiplication kernel. In this kernel, I avoided using shared memory so that I could have a very simple and clear WMMA implementation. However, in reality, we can make use of the same collaborative shared memory loading technique from our prior kernel to improve the arithmetic intensity of our WMMA kernel even further. I will do exactly this (among other improvements) in subsequent sections. The other aspect that I observed with this kernel is that despite having the same arithmetic intensity as our tiled matrix multiplication, it is significantly faster. This is because WMMA is a hardware-native operation. In the section introduction, we discussed the anatomy of an WMMA operation and why it is so fast, but I’ll call out a few ways the arithmetic intensity here is misleading. First, although it is standard to count multiplication and addition as separate FLOPs, they are fused into a single operation on the hardware when using tensor cores. Second, we discussed that WMMA fragments live on registers instead of shared memory. This is not reflected in our arithmetic intensity (which only takes into account global memory accesses). After accessing global memory in our tiled GEMM kernel, we have just transferred it to shared memory, so we still have to pull our data again from shared memory to our compute cores. Here, we load from global memory directly to the registers of the Tensor Core.\n\n\nBenchmarks\n\n\n\n\n\n\n\n\n\n\nGPU Model\nMemory Bandwidth\nPeak FP16 Compute\nRidge Point (FLOP/Byte)\nRuntime (ms)\n\n\n\n\nNVIDIA T4\n320 GB/s\n65 TFLOPS\n203\n1.68\n\n\nNVIDIA A100 (80GB)\n2,039 GB/s\n312 TFLOPS\n153\n0.17\n\n\nNVIDIA H100 (SXM)\n3,350 GB/s\n989 TFLOPS\n295\n0.10\n\n\nNVIDIA H200 (SXM)\n4,800 GB/s\n989 TFLOPS\n206\n0.10\n\n\nNVIDIA B200\n8,000 GB/s\n2,500 TFLOPS\n312\n0.10"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#double-buffer",
    "href": "posts/001-gemm-optimization/index.html#double-buffer",
    "title": "Note 001: GEMM Optimization",
    "section": "4. Double Buffer",
    "text": "4. Double Buffer\nThe next improvement we can make to our kernel is the use of a double buffer. The goal of a double buffer is to hide the latency of fetching data from global memory. In our current implementation, when threads request data from global memory, the compute cores have to pause while we wait for the data to arrive. Then we start computing, but our memory units are now sitting idle. When we’re done, we request data again and repeat the cycle. At any given time, either our compute cores or memory units are sitting idle.\nInstead, before we compute the current tile, we can issue an asynchronous request to load data for the next tile. Then our memory bus will load data in for the next tile while we compute the current tile. There is a dedicated hardware unit in the GPU that handles this asynchronous loading, the Async Copy Engine.\nThe double buffer is so named because we declare shared memory that is double the size of what we need to compute on. That way, we can use half of the buffer to load the next tiles of A and B from global memory to shared memory, and the other half of the buffer holds the currently loaded data that we feed to our Tensor Cores. We can track which half of the buffer is ready and which is being loaded. So our process is as follows within each loop iteration:\n\nAsynchronously request data for the next tile to the half of the buffer we are not about to use.\nWMMA compute on the current tile, using the half of the buffer that is ready.\nBarrier wait until the asynchronous request is complete. Then swap the stage index that tells us which half of the buffer is ready, and proceed to the next loop iteration.\n\nThere are a few other optimizations related to the data loading and grid configuration that we’ll pack into this kernel that warrant some explanation ahead of time. First, we will have each block be composed of 4 warps in a 2 x 2 grid (so 128 total threads). Each warp will be responsible for computing a 32 x 32 output tile of C, so in total one block will compute a 64 x 64 output tile.\nTo accomplish this, we will still loop over the K-dimension in a wide row in A and wide column in B, just as pictured in the image from tiled matrix multiplication. However, we will specify the wide row in A to have 64 rows, and the wide column in B to have 64 columns. We still loop over K via increments of 16 at a time. So in each loop iteration over K, we will use a 64 x 16 chunk of A and a 16 x 64 chunk of B. This is the same process as tiled matrix multiplication, but we are now using a non-square tile.\nBecause we have a 2 x 2 grid of warps, each warp will use a 32 x 16 chunk of A and a 16 x 32 chunk of B, and perform 4 WMMA operations (since they only take matrices of size 16 x 16). We then add their output to our accumulator fragments (4 for each warp, since each WMMA operation accumulates to a different 16 x 16 output tile) in each loop iteration. By the time our K loop is complete, our block has fully computed the value of \\(A \\times B\\) for a 64 x 64 tile of C.\nThe reason we do this is similar to why we loaded to shared memory in our tiled GEMM: we want to avoid redundant data loading and load as much data from global memory at once as we can usefully share across our block. By arranging our warps in 2 x 2 grid, we also are able to reuse more memory than if they were arranged in a straight line. For the collaborative data loading, we will use the thread ID in the block to determine what part of the current A (64 x 16) and B (16 x 64) chunks this thread will load. Each of these chunks can be treated as 128 8-half vectors, so each thread should load 8 elements. To reduce the number of instructions to load from global memory, we will employ vectorized loads to load 8 halves at once. Therefore, our A chunk can be viewed as 64 rows of 2 vectors, and our B chunk can be viewed as 8 rows of 8 vectors. We will use a vectorized store to global memory in the final section too, when possible.\n\nAnnotated Code\n#include &lt;cuda_runtime.h&gt;\n#include &lt;cuda_fp16.h&gt;\n#include &lt;mma.h&gt;\n#include &lt;cuda_pipeline_primitives.h&gt;\n\nusing namespace nvcuda;\n\n// ------------- CONFIGURATION -------------\nconstexpr int BLOCK_M = 64;\nconstexpr int BLOCK_N = 64; // One block computes a 64 x 64 tile of the output matrix\nconstexpr int BLOCK_K = 16; // Accumulation step\nconstexpr int WARP_SIZE = 32;\nconstexpr int THREAD_COUNT = 128;\nconstexpr int WMMA = 16;\n\n__global__ void gemm_buffer_kernel(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n\n    // ------------- INDEX CALCULATIONS -------------\n    // Linear view for data loading: which worker out of 128 threads am I?\n    int tid = threadIdx.x;\n\n    // Global position: what tile of the output matrix am I calculating?\n    int block_row_start = blockIdx.y * BLOCK_M;\n    int block_col_start = blockIdx.x * BLOCK_N;\n\n    // What warp am I in the 2x2 grid?\n1    int warp_id = tid / WARP_SIZE;\n    int warp_row = (warp_id / 2) * 32;\n    int warp_col = (warp_id % 2) * 32;\n\n2\n    // A tile: 64 x 16. Each row has 2 8-element vectors. \n    int row_A = tid / 2;       // 0 to 63\n    int col_A = (tid % 2) * 8; // 0 or 8\n    // B tile: 16 x 64. Each row has 8 8-element vectors. \n    int row_B = tid / 8;       // 0 to 7\n    int col_B = (tid % 8) * 8; // 0, 8, 16, 24, 32, 40, 48, or 56\n    // ----------------------------------------------\n\n\n    // ------------- MEMORY INITIALIZATION ----------\n    // Double Buffer: Shared Memory\n    __shared__ half sA[2][BLOCK_M * BLOCK_K]; // 64 rows, 16 cols (K)\n    __shared__ half sB[2][BLOCK_K * BLOCK_N]; // 16 rows (K), 64 cols\n\n    // Declare fragments and initialize accumulator. \n    wmma::fragment&lt;wmma::matrix_a, WMMA, WMMA, WMMA, half, wmma::row_major&gt; a_frag;\n    wmma::fragment&lt;wmma::matrix_b, WMMA, WMMA, WMMA, half, wmma::row_major&gt; b_frag;\n3    wmma::fragment&lt;wmma::accumulator, WMMA, WMMA, WMMA, float&gt; accum_frag[2][2];\n\n    #pragma unroll\n    for (int i = 0; i &lt; 2; i++) {\n        #pragma unroll\n        for (int j = 0; j &lt; 2; j++) {\n            wmma::fill_fragment(accum_frag[i][j], 0.0f);\n        }\n    }\n\n    // Pipeline setup\n    int stage = 0; // Alternates between 0 and 1\n    // ----------------------------------------------\n\n\n    // ------------- PROLOGUE -------------\n    // Load the first tile. \n    {\n        const half* src_A = A + (block_row_start + row_A) * K + (0 + col_A);\n        half* dst_A = &sA[stage][row_A * BLOCK_K + col_A];\n\n        const half* src_B = B + (0 + row_B) * N + (block_col_start + col_B);\n        half* dst_B = &sB[stage][row_B * BLOCK_N + col_B];\n\n        // Async copy. int4 is the size of 8 half elements\n4        __pipeline_memcpy_async(dst_A, src_A, sizeof(int4));\n        __pipeline_memcpy_async(dst_B, src_B, sizeof(int4)); \n\n5        __pipeline_commit();\n6        __pipeline_wait_prior(0);\n        __syncthreads();\n    }\n    // ------------------------------------\n\n    // ------------- MAIN LOOP -------------\n    #pragma unroll\n    for (int k = 0; k &lt; K; k += BLOCK_K) {\n\n        int k_next = k + BLOCK_K;\n\n        // 1. LOAD the next tile asynchronously\n        if (k_next &lt; K) {\n            // Turns 1 into 0 or 0 into 1\n            int next_stage = 1 - stage;\n\n            const half* src_A = A + (block_row_start + row_A) * K + (k_next + col_A);\n7            half* dst_A = &sA[next_stage][row_A * BLOCK_K + col_A];\n            \n            const half* src_B = B + (k_next + row_B) * N + (block_col_start + col_B);\n            half* dst_B = &sB[next_stage][row_B * BLOCK_N + col_B];\n\n            __pipeline_memcpy_async(dst_A, src_A, sizeof(int4)); \n            __pipeline_memcpy_async(dst_B, src_B, sizeof(int4)); \n\n            __pipeline_commit();\n        }\n\n        // 2. MATH: process the current tile. Recall we have a 2 x 2 grid of 16 x 16 subtiles for each warp.\n        #pragma unroll\n        for (int i = 0; i &lt; 2; i++) {\n            #pragma unroll\n            for (int j = 0; j &lt; 2; j++) {\n                // Calculate pointer into shared memory for this sub-tile\n                int smem_row = warp_row + (i * 16);\n                int smem_col = warp_col + (j * 16);\n\n                // Load fragments from shared memory\n8                half* tile_ptr_A = &sA[stage][smem_row * BLOCK_K];\n                half* tile_ptr_B = &sB[stage][smem_col];\n\n                wmma::load_matrix_sync(a_frag, tile_ptr_A, BLOCK_K);\n                wmma::load_matrix_sync(b_frag, tile_ptr_B, BLOCK_N);\n\n                // Multiply matrices and accumulate\n                wmma::mma_sync(accum_frag[i][j], a_frag, b_frag, accum_frag[i][j]);\n            }\n        }\n\n        // 3. WAIT for next tile\n        if (k + BLOCK_K &lt; K) {\n9            __pipeline_wait_prior(0);\n            __syncthreads();\n            stage = 1 - stage;\n        }\n    }\n    // ------------------------------------\n\n    __syncthreads(); // Since the syncthreads above won't execute on the last iteration\n   \n    // ------- EPILOGUE: Store C ----------\n    // Size: 64 * 64 floats = 64 * 64 * 4 bytes = 16 KB. Fits easily in modern L1/Shared\n    __shared__ float sC[BLOCK_M * BLOCK_N];\n\n    // Warps dump their fragments to shared memory, one 16x16 subtile at a time.\n    #pragma unroll\n    for (int i = 0; i &lt; 2; i++) {\n        #pragma unroll\n        for (int j = 0; j &lt; 2; j++) {\n            float* subtile_ptr = sC + (warp_row + i * 16) * BLOCK_N + (warp_col + j * 16);\n            wmma::store_matrix_sync(subtile_ptr, accum_frag[i][j], BLOCK_N, wmma::mem_row_major);\n        }\n    }\n\n    // Wait for all threads to write to sC\n    __syncthreads();\n\n    #pragma unroll\n10    for (int i = tid * 8; i &lt; BLOCK_M * BLOCK_N; i += THREAD_COUNT * 8) {\n        int row = i / BLOCK_N;\n        int col = i % BLOCK_N;\n\n        int global_row = block_row_start + row;\n        int global_col = block_col_start + col;\n\n        half buffer[8];\n\n        // Boundary check\n        if (global_row &lt; M && (global_col + 7) &lt; N) {\n            #pragma unroll\n            for (int j = 0; j &lt; 8; j++) {\n                float val = alpha * sC[i + j];\n\n                if (beta != 0.0f) {\n                    float old_c = __half2float(C[global_row * N + global_col + j]);\n                    val += beta * old_c;\n                } \n\n                buffer[j] = __float2half(val);\n            }\n            \n            // Vectorized store\n            *(int4*)&C[global_row * N + global_col] = *(int4*)buffer;\n\n        } else {\n            #pragma unroll\n            for (int j = 0; j &lt; 8; j++) {\n                if (global_row &lt; M && (global_col + j) &lt; N) {\n                    int out_idx = global_row * N + global_col + j;\n\n                    float val = alpha * sC[i + j];\n\n                    if (beta != 0.0f) {\n                        float old_c = __half2float(C[out_idx]);\n                        val += beta * old_c;\n                    } \n\n                    C[out_idx] = __float2half(val);\n                }\n            }\n        }\n    }\n}\n\n\n// Same as in Tiled Matrix Multiplication\n__global__ void gemm_tiled_kernel(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) { ... }\n\n\nextern \"C\" void solve(const half* A, const half* B, half* C, int M, int N, int K, float alpha, float beta) {\n    \n    if (M % 64 == 0 && N % 64 == 0 && K % 16 == 0) {\n        \n        dim3 blockDim(THREAD_COUNT);\n        dim3 gridDim(N / BLOCK_N, M / BLOCK_M);\n        gemm_buffer_kernel&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n\n    } else {\n\n        dim3 block(TILE_WIDTH, TILE_WIDTH);\n        dim3 grid(\n            (N + (TILE_WIDTH * COARSE_FACTOR) - 1) / (TILE_WIDTH * COARSE_FACTOR), \n            (M + TILE_WIDTH - 1) / TILE_WIDTH\n        );\n\n        gemm_tiled_kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, M, N, K, alpha, beta);\n\n    }\n}\n\n1\n\nWarp Grid: As we have 128 threads per block, we have 4 warps per block, which we arrange in a 2x2 grid. Each block computes a 64 x 64 output tile, so we need to assign each warp a 32 x 32 output tile.\n\n2\n\nData Loading: We treat the A and B tiles, 64 x 16 and 16 x 64 respectively, as linear arrays of 128 8-element vectors. So each thread is responsible for loading 8 halves to shared memory.\n\n3\n\nAccumulator Grid: Accumulator is a 2 x 2 grid because each warp is assigned a 32 x 32 output tile but can only compute 16 x 16 at a time.\n\n4\n\n__pipeline_memcpy_async: Instructs the Async Copy Engine to copy data from global memory to shared memory. As this is an asynchronous operation, the command returns immediately and allows us to continue with other instructions while the memory loads. We issue a vectorized load for 8 halves worth of data at once (sizeof(int4)).\n\n5\n\n__pipeline_commit: Marks the end of a batch of copy commands. Effectively, memcpy_async adds the copy instruction to our shopping cart, and commit places the order.\n\n6\n\n__pipeline_wait_prior: Pauses the thread execution until the specified number of committed batches are incomplete. Since we pass in 0, we are pausing thread execution until all asynchronous loads that were issued are complete (in our case, only a single load). In any case, after this line, we have to issue syncthreads because each thread is collaboratively loading a piece of A and B that every thread will need for compute.\n\n7\n\nWriting to the Double Buffer: We load into the half of the double buffer that we’re not using this loop iterationn.\n\n8\n\nReading from the Double Buffer: We pull data for the WMMA operation from the half of the double buffer that is ready.\n\n9\n\nNotice the location of this command in the main loop compared to in the prologue. We only had to issue it immediately after placing the copy command in the prologue because we needed to load the very first tile for compute. In the main loop, we don’t need to hold up threads on the copy completion until we have finished all compute for this iteration.\n\n10\n\nVectorized Store: In this loop, we complete our GEMM operation by taking our accumulated result of A x B, scaling it by alpha, adding it to beta * C, and finally storing it in global memory. We have 64 * 64 = 4096 total elements to process and store, and 128 threads to do this. So we must process 32 elements per thread. If we vectorize this into processing 8 elements per step, we need only 4 steps per thread. However, we have an else block here that covers the tail elements once we have fewer than 8 elements left and can’t do a vectorized store.\n\n\n\n\nArithmetic Intensity\nWe will examine a single iteration of the main loop. We load in a 64 x 16 chunk of A and a 16 x 64 chunk of B from global memory, for a total of 2,048 halves, which is 4,096 bytes. Our output tile is 64 x 64, and on each loop iteration, we accumulate a dot product of two 16-element vectors to each pixel of the output tile. This dot product consists of 16 multiplications and 16 additions, so 32 FLOPs per pixel. In total then, we perform 64 * 64 * 32 = 131,072 FLOPs per loop iteration. Dividing this out by our global memory load of 4,096 bytes, we get an arithmetic intensity of 32 FLOPs/B. This is due to our increased tile size, not due to our double buffer which mainly helps with hiding memory latency. So we should theoretically have two different improvements that speed up our runtime: reusing more data due to the larger tile size, and latency hiding due to the double buffer. Thankfully, the runtime confirms this, as we can see considerable speedup on all GPUs.\n\n\nBenchmarks\n\n\n\n\n\n\n\n\n\n\nGPU Model\nMemory Bandwidth\nPeak FP16 Compute\nRidge Point (FLOP/Byte)\nRuntime (ms)\n\n\n\n\nNVIDIA T4\n320 GB/s\n65 TFLOPS\n203\n1.04\n\n\nNVIDIA A100 (80GB)\n2,039 GB/s\n312 TFLOPS\n153\n0.12\n\n\nNVIDIA H100 (SXM)\n3,350 GB/s\n989 TFLOPS\n295\n0.05\n\n\nNVIDIA H200 (SXM)\n4,800 GB/s\n989 TFLOPS\n206\n0.05\n\n\nNVIDIA B200\n8,000 GB/s\n2,500 TFLOPS\n312\n0.05"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#swizzling",
    "href": "posts/001-gemm-optimization/index.html#swizzling",
    "title": "Note 001: GEMM Optimization",
    "section": "5. Swizzling",
    "text": "5. Swizzling\n\nAnnotated Code\n\n\nArithmetic Intensity\n\n\nBenchmarks"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#arbitrary-matrix-dimensions",
    "href": "posts/001-gemm-optimization/index.html#arbitrary-matrix-dimensions",
    "title": "Note 001: GEMM Optimization",
    "section": "6. Arbitrary Matrix Dimensions",
    "text": "6. Arbitrary Matrix Dimensions\n\nAnnotated Code\n\n\nArithmetic Intensity\n\n\nBenchmarks"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#further-optimizations",
    "href": "posts/001-gemm-optimization/index.html#further-optimizations",
    "title": "Note 001: GEMM Optimization",
    "section": "Further Optimizations",
    "text": "Further Optimizations"
  },
  {
    "objectID": "posts/001-gemm-optimization/index.html#additional-reading",
    "href": "posts/001-gemm-optimization/index.html#additional-reading",
    "title": "Note 001: GEMM Optimization",
    "section": "Additional Reading",
    "text": "Additional Reading"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rohan Reddy / Notes",
    "section": "",
    "text": "Rohan Reddy is a software engineer based in New York City.\nRead more about me →"
  },
  {
    "objectID": "index.html#recent-notes",
    "href": "index.html#recent-notes",
    "title": "Rohan Reddy / Notes",
    "section": "Recent Notes",
    "text": "Recent Notes"
  }
]